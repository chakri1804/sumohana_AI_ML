{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give Number of Hidden Layer nodes :\n",
      "2\n",
      "Give number of training samples per bit pair:\n",
      "10\n",
      "Give std.dev for noise :\n",
      "0.00001\n",
      "Give number of Epochs :\n",
      "10\n",
      "Give learning rate :\n",
      "0.001\n",
      "Give the operation you need to train (Enter text in CAPS):\n",
      "1. XOR\n",
      "2. OR\n",
      "3. AND\n",
      "XOR\n",
      "[[0.05243218]]\n",
      "[[0.05280993]]\n",
      "[[0.05318947]]\n",
      "[[0.0535708]]\n",
      "[[0.0539539]]\n",
      "[[0.05433877]]\n",
      "[[0.05472538]]\n",
      "[[0.05511374]]\n",
      "[[0.05550383]]\n",
      "[[0.05589565]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "print(\"Give Number of Hidden Layer nodes :\")\n",
    "M = int(input())\n",
    "print(\"Give number of training samples per bit pair:\")\n",
    "n = int(input())\n",
    "print(\"Give std.dev for noise :\")\n",
    "noise = float(input())\n",
    "print(\"Give number of Epochs :\")\n",
    "epochs = int(input())\n",
    "print(\"Give learning rate :\")\n",
    "lr = float(input())\n",
    "print(\"Give the operation you need to train (Enter text in CAPS):\")\n",
    "print(\"1. XOR\")\n",
    "print(\"2. OR\")\n",
    "print(\"3. AND\")\n",
    "option = input()\n",
    "# Generate Datasets\n",
    "\n",
    "Input = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "if option =='XOR':\n",
    "    Output = np.array([[0],[1],[1],[0]])\n",
    "if option =='OR':\n",
    "    Output = np.array([[0],[1],[1],[1]])\n",
    "if option =='AND':\n",
    "    Output = np.array([[0],[0],[0],[1]])\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(Input)):\n",
    "    for j in range(n):\n",
    "        X.append(Input[i]+np.random.normal(0,noise,(1,2)))\n",
    "        Y.append(Output[i]+np.random.normal(0,noise))\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.reshape((len(Y),2))\n",
    "Y = np.array(Y)\n",
    "\n",
    "# defining functions\n",
    "\n",
    "def sigm(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def diff_sigm(x):\n",
    "    return (sigm(x)-sigm(x)**2)\n",
    "\n",
    "def layer(x,W,b):\n",
    "    return (np.matmul(W.T,x.reshape(len(x),1)) + b)\n",
    "\n",
    "def sq_err(y,Y):\n",
    "    return (y-Y)**2\n",
    "# Initializing weights\n",
    "\n",
    "W1 = np.random.normal(0,1,(2,M))\n",
    "Bi1 = np.random.normal(0,1,(M,1))\n",
    "W2 = np.random.normal(0,1,(M,1))\n",
    "Bi2 = np.random.normal(0,1,(1,1))\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# training\n",
    "loss_plot = []\n",
    "for i in range(epochs):\n",
    "    w1 = np.zeros(W1.shape)\n",
    "    b1 = np.zeros(Bi1.shape)\n",
    "    w2 = np.zeros(W2.shape)\n",
    "    b2 = np.zeros(Bi2.shape)\n",
    "    for j in range(len(Y)):\n",
    "        #forward path\n",
    "        out1 = layer(X[j],W1,Bi1)\n",
    "#         print(out1)\n",
    "        z = sigm(out1)\n",
    "#         print(z)\n",
    "        out2 = layer(z,W2,Bi2)\n",
    "        y = sigm(out2)\n",
    "#         print(y)\n",
    "        #backpropagation\n",
    "        b2 += 2*(y-Y[j])*diff_sigm(out2)\n",
    "        w2 += 2*(y-Y[j])*diff_sigm(out2)*z\n",
    "        loss = sq_err(y,Y[j])\n",
    "        loss_plot.append(loss)\n",
    "        for k in range(M):\n",
    "            b1[k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]).reshape(1,)\n",
    "            w1[:,k] += (2*(y-Y[j])*diff_sigm(out2)*diff_sigm(out1[k])*W2[k]*X[j]).reshape(2,)\n",
    "    print(loss)\n",
    "    W1 -= lr*w1\n",
    "    W2 -= lr*w2\n",
    "    Bi1 -= lr*b1\n",
    "    Bi2 -= lr*b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0812237  0.081233   0.08122679 0.08122356 0.08122294 0.08123714\n",
      " 0.08122462 0.08121674 0.0812209  0.08122549 0.49687859 0.49688727\n",
      " 0.49689786 0.49689227 0.49686151 0.49689614 0.49690511 0.49689921\n",
      " 0.49688349 0.49689319 0.58800937 0.58803686 0.58802994 0.58802111\n",
      " 0.58801578 0.58801668 0.58800326 0.58800633 0.5880066  0.58801435\n",
      " 0.05242793 0.05243308 0.0524346  0.05242982 0.05243446 0.05244125\n",
      " 0.0524375  0.05242809 0.05243135 0.05243218 0.0817362  0.08174554\n",
      " 0.08173931 0.08173606 0.08173544 0.08174969 0.08173713 0.08172922\n",
      " 0.08173339 0.08173801 0.49541656 0.49542523 0.49543581 0.49543022\n",
      " 0.4953995  0.49543408 0.49544304 0.49543714 0.49542145 0.49543114\n",
      " 0.58687016 0.58689763 0.58689071 0.58688188 0.58687657 0.58687746\n",
      " 0.58686405 0.58686712 0.58686739 0.58687513 0.05280566 0.05281083\n",
      " 0.05281236 0.05280756 0.05281222 0.05281903 0.05281527 0.05280583\n",
      " 0.05280909 0.05280993 0.08225058 0.08225995 0.0822537  0.08225044\n",
      " 0.08224982 0.08226411 0.08225151 0.08224358 0.08224776 0.08225239\n",
      " 0.49395556 0.49396421 0.49397477 0.4939692  0.49393852 0.49397304\n",
      " 0.493982   0.49397609 0.49396044 0.49397011 0.58573122 0.58575867\n",
      " 0.58575175 0.58574293 0.58573762 0.58573852 0.58572512 0.58572819\n",
      " 0.58572845 0.58573618 0.05318519 0.05319038 0.05319191 0.05318709\n",
      " 0.05319177 0.05319861 0.05319483 0.05318535 0.05318863 0.05318947\n",
      " 0.08276681 0.08277621 0.08276994 0.08276667 0.08276604 0.08278039\n",
      " 0.08276774 0.08275979 0.08276398 0.08276862 0.49249564 0.49250428\n",
      " 0.49251483 0.49250927 0.49247862 0.49251309 0.49252204 0.49251614\n",
      " 0.49250051 0.49251017 0.5845926  0.58462003 0.5846131  0.5846043\n",
      " 0.584599   0.58459989 0.58458651 0.58458958 0.58458984 0.58459756\n",
      " 0.0535665  0.05357171 0.05357324 0.05356841 0.0535731  0.05357997\n",
      " 0.05357617 0.05356666 0.05356995 0.0535708  0.08328487 0.08329431\n",
      " 0.08328802 0.08328473 0.08328411 0.0832985  0.08328581 0.08327783\n",
      " 0.08328204 0.08328669 0.49103687 0.49104551 0.49105604 0.49105049\n",
      " 0.49101988 0.4910543  0.49106323 0.49105733 0.49104174 0.49105139\n",
      " 0.58345435 0.58348175 0.58347482 0.58346603 0.58346074 0.58346163\n",
      " 0.58344826 0.58345132 0.58345158 0.58345929 0.05394958 0.05395481\n",
      " 0.05395635 0.0539515  0.05395621 0.0539631  0.05395929 0.05394975\n",
      " 0.05395305 0.0539539  0.08380476 0.08381422 0.08380791 0.08380461\n",
      " 0.08380398 0.08381843 0.08380569 0.08379769 0.08380192 0.08380658\n",
      " 0.48957932 0.48958795 0.48959846 0.48959292 0.48956235 0.48959672\n",
      " 0.48960564 0.48959975 0.48958418 0.48959382 0.5823165  0.58234388\n",
      " 0.58233695 0.58232817 0.58232288 0.58232377 0.58231041 0.58231348\n",
      " 0.58231374 0.58232144 0.05433443 0.05433967 0.05434122 0.05433636\n",
      " 0.05434108 0.054348   0.05434417 0.05433459 0.05433791 0.05433877\n",
      " 0.08432644 0.08433594 0.0843296  0.08432629 0.08432566 0.08434015\n",
      " 0.08432737 0.08431935 0.08432359 0.08432826 0.48812305 0.48813166\n",
      " 0.48814217 0.48813663 0.4881061  0.48814042 0.48814933 0.48814344\n",
      " 0.4881279  0.48813752 0.5811791  0.58120647 0.58119953 0.58119076\n",
      " 0.58118548 0.58118637 0.58117302 0.58117609 0.58117635 0.58118404\n",
      " 0.05472103 0.05472629 0.05472785 0.05472296 0.0547277  0.05473464\n",
      " 0.0547308  0.05472119 0.05472452 0.05472538 0.0848499  0.08485943\n",
      " 0.08485308 0.08484975 0.08484912 0.08486366 0.08485083 0.08484279\n",
      " 0.08484704 0.08485173 0.48666813 0.48667672 0.48668721 0.48668169\n",
      " 0.4866512  0.48668546 0.48669437 0.48668847 0.48667296 0.48668258\n",
      " 0.5800422  0.58006955 0.58006261 0.58005384 0.58004858 0.58004946\n",
      " 0.58003613 0.5800392  0.58003945 0.58004713 0.05510937 0.05511465\n",
      " 0.05511621 0.05511131 0.05511607 0.05512304 0.05511918 0.05510953\n",
      " 0.05511287 0.05511374 0.08537512 0.08538468 0.08537831 0.08537497\n",
      " 0.08537434 0.08538892 0.08537606 0.08536799 0.08537225 0.08537696\n",
      " 0.4852146  0.48522319 0.48523366 0.48522815 0.4851977  0.48523191\n",
      " 0.4852408  0.48523491 0.48521943 0.48522903 0.57890585 0.57893317\n",
      " 0.57892623 0.57891747 0.57891222 0.5789131  0.57889978 0.57890284\n",
      " 0.5789031  0.57891077 0.05549944 0.05550475 0.05550631 0.05550139\n",
      " 0.05550616 0.05551316 0.05550929 0.05549961 0.05550296 0.05550383\n",
      " 0.08590208 0.08591168 0.08590529 0.08590193 0.0859013  0.08591593\n",
      " 0.08590302 0.08589493 0.08589921 0.08590392 0.48376255 0.48377112\n",
      " 0.48378158 0.48377608 0.48374566 0.48377982 0.48378871 0.48378282\n",
      " 0.48376737 0.48377695 0.57777008 0.57779738 0.57779043 0.57778169\n",
      " 0.57777644 0.57777732 0.57776401 0.57776708 0.57776733 0.57777499\n",
      " 0.05589123 0.05589656 0.05589813 0.05589319 0.05589798 0.055905\n",
      " 0.05590111 0.0558914  0.05589476 0.05589565]\n"
     ]
    }
   ],
   "source": [
    "# loss_plot = np.array(loss_plot)\n",
    "loss_plot = (np.array(loss_plot)).reshape((len(loss_plot),))\n",
    "print(loss_plot)\n",
    "plt.plot(loss_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
