{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [231.2283843]\n",
      "Epoch: 1 Loss: [103.10987612]\n",
      "Epoch: 2 Loss: [101.00093985]\n",
      "Epoch: 3 Loss: [88.32669506]\n",
      "Epoch: 4 Loss: [73.18538318]\n",
      "Epoch: 5 Loss: [60.66840056]\n",
      "Epoch: 6 Loss: [52.42852421]\n",
      "Epoch: 7 Loss: [47.23640463]\n",
      "Epoch: 8 Loss: [43.51131435]\n",
      "Epoch: 9 Loss: [40.92984885]\n",
      "Epoch: 10 Loss: [39.03442324]\n",
      "Epoch: 11 Loss: [37.76094229]\n",
      "Epoch: 12 Loss: [36.50238026]\n",
      "Epoch: 13 Loss: [35.53039102]\n",
      "Epoch: 14 Loss: [34.66170121]\n",
      "Epoch: 15 Loss: [33.83810742]\n",
      "Epoch: 16 Loss: [33.04749423]\n",
      "Epoch: 17 Loss: [32.32842439]\n",
      "Epoch: 18 Loss: [31.64623673]\n",
      "Epoch: 19 Loss: [30.95173646]\n",
      "Epoch: 20 Loss: [30.28795879]\n",
      "Epoch: 21 Loss: [29.62178451]\n",
      "Epoch: 22 Loss: [28.97780683]\n",
      "Epoch: 23 Loss: [28.34059338]\n",
      "Epoch: 24 Loss: [27.72164735]\n",
      "Epoch: 25 Loss: [27.12366513]\n",
      "Epoch: 26 Loss: [26.55804361]\n",
      "Epoch: 27 Loss: [26.01085894]\n",
      "Epoch: 28 Loss: [25.51217803]\n",
      "Epoch: 29 Loss: [25.02941212]\n",
      "Epoch: 30 Loss: [24.59099766]\n",
      "Epoch: 31 Loss: [24.16816086]\n",
      "Epoch: 32 Loss: [23.7903573]\n",
      "Epoch: 33 Loss: [23.43464947]\n",
      "Epoch: 34 Loss: [23.1120623]\n",
      "Epoch: 35 Loss: [22.81284597]\n",
      "Epoch: 36 Loss: [22.53623047]\n",
      "Epoch: 37 Loss: [22.27746294]\n",
      "Epoch: 38 Loss: [22.03215638]\n",
      "Epoch: 39 Loss: [21.79669069]\n",
      "Epoch: 40 Loss: [21.56697955]\n",
      "Epoch: 41 Loss: [21.33980886]\n",
      "Epoch: 42 Loss: [21.1122323]\n",
      "Epoch: 43 Loss: [20.88161247]\n",
      "Epoch: 44 Loss: [20.64579241]\n",
      "Epoch: 45 Loss: [20.40294917]\n",
      "Epoch: 46 Loss: [20.1515203]\n",
      "Epoch: 47 Loss: [19.89017807]\n",
      "Epoch: 48 Loss: [19.61836692]\n",
      "Epoch: 49 Loss: [19.33039014]\n",
      "Epoch: 50 Loss: [19.03891562]\n",
      "Epoch: 51 Loss: [18.72818358]\n",
      "Epoch: 52 Loss: [18.40278052]\n",
      "Epoch: 53 Loss: [18.06248983]\n",
      "Epoch: 54 Loss: [17.70410537]\n",
      "Epoch: 55 Loss: [17.32702806]\n",
      "Epoch: 56 Loss: [16.92950947]\n",
      "Epoch: 57 Loss: [16.51142907]\n",
      "Epoch: 58 Loss: [16.07003421]\n",
      "Epoch: 59 Loss: [15.60334857]\n",
      "Epoch: 60 Loss: [15.10989534]\n",
      "Epoch: 61 Loss: [14.58819819]\n",
      "Epoch: 62 Loss: [14.03721793]\n",
      "Epoch: 63 Loss: [13.45742276]\n",
      "Epoch: 64 Loss: [12.85014058]\n",
      "Epoch: 65 Loss: [12.21923093]\n",
      "Epoch: 66 Loss: [11.57181524]\n",
      "Epoch: 67 Loss: [10.91810823]\n",
      "Epoch: 68 Loss: [10.27084597]\n",
      "Epoch: 69 Loss: [9.64327272]\n",
      "Epoch: 70 Loss: [9.04685315]\n",
      "Epoch: 71 Loss: [8.48902325]\n",
      "Epoch: 72 Loss: [7.97272833]\n",
      "Epoch: 73 Loss: [7.497725]\n",
      "Epoch: 74 Loss: [7.06206523]\n",
      "Epoch: 75 Loss: [6.6629864]\n",
      "Epoch: 76 Loss: [6.29737997]\n",
      "Epoch: 77 Loss: [5.96211991]\n",
      "Epoch: 78 Loss: [5.65425072]\n",
      "Epoch: 79 Loss: [5.37101558]\n",
      "Epoch: 80 Loss: [5.11006624]\n",
      "Epoch: 81 Loss: [4.86904324]\n",
      "Epoch: 82 Loss: [4.6460464]\n",
      "Epoch: 83 Loss: [4.43934323]\n",
      "Epoch: 84 Loss: [4.24735764]\n",
      "Epoch: 85 Loss: [4.068682]\n",
      "Epoch: 86 Loss: [3.90220903]\n",
      "Epoch: 87 Loss: [3.74667149]\n",
      "Epoch: 88 Loss: [3.6012483]\n",
      "Epoch: 89 Loss: [3.46497784]\n",
      "Epoch: 90 Loss: [3.33715128]\n",
      "Epoch: 91 Loss: [3.21701669]\n",
      "Epoch: 92 Loss: [3.10399453]\n",
      "Epoch: 93 Loss: [2.997479]\n",
      "Epoch: 94 Loss: [2.89700155]\n",
      "Epoch: 95 Loss: [2.80206393]\n",
      "Epoch: 96 Loss: [2.71230085]\n",
      "Epoch: 97 Loss: [2.62724976]\n",
      "Epoch: 98 Loss: [2.5466844]\n",
      "Epoch: 99 Loss: [2.47017793]\n",
      "Epoch: 100 Loss: [2.39753592]\n",
      "Epoch: 101 Loss: [2.3284298]\n",
      "Epoch: 102 Loss: [2.26267469]\n",
      "Epoch: 103 Loss: [2.20000354]\n",
      "Epoch: 104 Loss: [2.14028151]\n",
      "Epoch: 105 Loss: [2.08322629]\n",
      "Epoch: 106 Loss: [2.02878981]\n",
      "Epoch: 107 Loss: [1.97668956]\n",
      "Epoch: 108 Loss: [1.926901]\n",
      "Epoch: 109 Loss: [1.87917179]\n",
      "Epoch: 110 Loss: [1.83350185]\n",
      "Epoch: 111 Loss: [1.78964049]\n",
      "Epoch: 112 Loss: [1.74764338]\n",
      "Epoch: 113 Loss: [1.70721599]\n",
      "Epoch: 114 Loss: [1.66849159]\n",
      "Epoch: 115 Loss: [1.63113444]\n",
      "Epoch: 116 Loss: [1.59533415]\n",
      "Epoch: 117 Loss: [1.5607374]\n",
      "Epoch: 118 Loss: [1.52755951]\n",
      "Epoch: 119 Loss: [1.4954366]\n",
      "Epoch: 120 Loss: [1.4646302]\n",
      "Epoch: 121 Loss: [1.43473755]\n",
      "Epoch: 122 Loss: [1.40606564]\n",
      "Epoch: 123 Loss: [1.37819794]\n",
      "Epoch: 124 Loss: [1.35145133]\n",
      "Epoch: 125 Loss: [1.32543029]\n",
      "Epoch: 126 Loss: [1.3004276]\n",
      "Epoch: 127 Loss: [1.27608564]\n",
      "Epoch: 128 Loss: [1.25266616]\n",
      "Epoch: 129 Loss: [1.22986256]\n",
      "Epoch: 130 Loss: [1.20788698]\n",
      "Epoch: 131 Loss: [1.1864851]\n",
      "Epoch: 132 Loss: [1.16583072]\n",
      "Epoch: 133 Loss: [1.14571571]\n",
      "Epoch: 134 Loss: [1.12627024]\n",
      "Epoch: 135 Loss: [1.10733685]\n",
      "Epoch: 136 Loss: [1.08900231]\n",
      "Epoch: 137 Loss: [1.07115464]\n",
      "Epoch: 138 Loss: [1.05384469]\n",
      "Epoch: 139 Loss: [1.03699385]\n",
      "Epoch: 140 Loss: [1.02063041]\n",
      "Epoch: 141 Loss: [1.00471101]\n",
      "Epoch: 142 Loss: [0.98922654]\n",
      "Epoch: 143 Loss: [0.97414337]\n",
      "Epoch: 144 Loss: [0.95947507]\n",
      "Epoch: 145 Loss: [0.94517801]\n",
      "Epoch: 146 Loss: [0.93126143]\n",
      "Epoch: 147 Loss: [0.91769344]\n",
      "Epoch: 148 Loss: [0.90447731]\n",
      "Epoch: 149 Loss: [0.89158742]\n",
      "Epoch: 150 Loss: [0.87901955]\n",
      "Epoch: 151 Loss: [0.8667623]\n",
      "Epoch: 152 Loss: [0.85480068]\n",
      "Epoch: 153 Loss: [0.84312984]\n",
      "Epoch: 154 Loss: [0.83173406]\n",
      "Epoch: 155 Loss: [0.8206115]\n",
      "Epoch: 156 Loss: [0.809746]\n",
      "Epoch: 157 Loss: [0.79913221]\n",
      "Epoch: 158 Loss: [0.78876415]\n",
      "Epoch: 159 Loss: [0.77862934]\n",
      "Epoch: 160 Loss: [0.76872241]\n",
      "Epoch: 161 Loss: [0.75903854]\n",
      "Epoch: 162 Loss: [0.74956644]\n",
      "Epoch: 163 Loss: [0.74030125]\n",
      "Epoch: 164 Loss: [0.73123917]\n",
      "Epoch: 165 Loss: [0.72236978]\n",
      "Epoch: 166 Loss: [0.7136888]\n",
      "Epoch: 167 Loss: [0.70519319]\n",
      "Epoch: 168 Loss: [0.6968734]\n",
      "Epoch: 169 Loss: [0.68872556]\n",
      "Epoch: 170 Loss: [0.68074734]\n",
      "Epoch: 171 Loss: [0.67292991]\n",
      "Epoch: 172 Loss: [0.66526978]\n",
      "Epoch: 173 Loss: [0.65776519]\n",
      "Epoch: 174 Loss: [0.65040801]\n",
      "Epoch: 175 Loss: [0.64319503]\n",
      "Epoch: 176 Loss: [0.636125]\n",
      "Epoch: 177 Loss: [0.62919042]\n",
      "Epoch: 178 Loss: [0.62238829]\n",
      "Epoch: 179 Loss: [0.61571782]\n",
      "Epoch: 180 Loss: [0.60917206]\n",
      "Epoch: 181 Loss: [0.60274821]\n",
      "Epoch: 182 Loss: [0.59644584]\n",
      "Epoch: 183 Loss: [0.59025855]\n",
      "Epoch: 184 Loss: [0.58418368]\n",
      "Epoch: 185 Loss: [0.5782211]\n",
      "Epoch: 186 Loss: [0.57236492]\n",
      "Epoch: 187 Loss: [0.56661257]\n",
      "Epoch: 188 Loss: [0.56096423]\n",
      "Epoch: 189 Loss: [0.55541444]\n",
      "Epoch: 190 Loss: [0.54996073]\n",
      "Epoch: 191 Loss: [0.5446038]\n",
      "Epoch: 192 Loss: [0.5393366]\n",
      "Epoch: 193 Loss: [0.53416252]\n",
      "Epoch: 194 Loss: [0.52907359]\n",
      "Epoch: 195 Loss: [0.52407093]\n",
      "Epoch: 196 Loss: [0.51915333]\n",
      "Epoch: 197 Loss: [0.51431645]\n",
      "Epoch: 198 Loss: [0.50955894]\n",
      "Epoch: 199 Loss: [0.5048802]\n",
      "Epoch: 200 Loss: [0.50027769]\n",
      "Epoch: 201 Loss: [0.49574846]\n",
      "Epoch: 202 Loss: [0.49129219]\n",
      "Epoch: 203 Loss: [0.48690777]\n",
      "Epoch: 204 Loss: [0.48259241]\n",
      "Epoch: 205 Loss: [0.47834384]\n",
      "Epoch: 206 Loss: [0.47416268]\n",
      "Epoch: 207 Loss: [0.47004655]\n",
      "Epoch: 208 Loss: [0.46599275]\n",
      "Epoch: 209 Loss: [0.46200123]\n",
      "Epoch: 210 Loss: [0.45807121]\n",
      "Epoch: 211 Loss: [0.45420031]\n",
      "Epoch: 212 Loss: [0.45038651]\n",
      "Epoch: 213 Loss: [0.44663073]\n",
      "Epoch: 214 Loss: [0.4429299]\n",
      "Epoch: 215 Loss: [0.43928375]\n",
      "Epoch: 216 Loss: [0.43569142]\n",
      "Epoch: 217 Loss: [0.43215046]\n",
      "Epoch: 218 Loss: [0.42866067]\n",
      "Epoch: 219 Loss: [0.42522174]\n",
      "Epoch: 220 Loss: [0.42183177]\n",
      "Epoch: 221 Loss: [0.41848894]\n",
      "Epoch: 222 Loss: [0.41519417]\n",
      "Epoch: 223 Loss: [0.41194506]\n",
      "Epoch: 224 Loss: [0.4087415]\n",
      "Epoch: 225 Loss: [0.40558191]\n",
      "Epoch: 226 Loss: [0.40246621]\n",
      "Epoch: 227 Loss: [0.3993927]\n",
      "Epoch: 228 Loss: [0.39636119]\n",
      "Epoch: 229 Loss: [0.39337102]\n",
      "Epoch: 230 Loss: [0.39042037]\n",
      "Epoch: 231 Loss: [0.38750909]\n",
      "Epoch: 232 Loss: [0.38463713]\n",
      "Epoch: 233 Loss: [0.38180309]\n",
      "Epoch: 234 Loss: [0.37900546]\n",
      "Epoch: 235 Loss: [0.37624507]\n",
      "Epoch: 236 Loss: [0.37352022]\n",
      "Epoch: 237 Loss: [0.37083076]\n",
      "Epoch: 238 Loss: [0.36817554]\n",
      "Epoch: 239 Loss: [0.36555454]\n",
      "Epoch: 240 Loss: [0.36296653]\n",
      "Epoch: 241 Loss: [0.36041147]\n",
      "Epoch: 242 Loss: [0.35788824]\n",
      "Epoch: 243 Loss: [0.35539664]\n",
      "Epoch: 244 Loss: [0.35293633]\n",
      "Epoch: 245 Loss: [0.35050591]\n",
      "Epoch: 246 Loss: [0.34810526]\n",
      "Epoch: 247 Loss: [0.34573448]\n",
      "Epoch: 248 Loss: [0.34339261]\n",
      "Epoch: 249 Loss: [0.34107853]\n",
      "Epoch: 250 Loss: [0.33879226]\n",
      "Epoch: 251 Loss: [0.33653386]\n",
      "Epoch: 252 Loss: [0.33430242]\n",
      "Epoch: 253 Loss: [0.33209678]\n",
      "Epoch: 254 Loss: [0.32991759]\n",
      "Epoch: 255 Loss: [0.32776381]\n",
      "Epoch: 256 Loss: [0.32563531]\n",
      "Epoch: 257 Loss: [0.32353129]\n",
      "Epoch: 258 Loss: [0.32145181]\n",
      "Epoch: 259 Loss: [0.31939601]\n",
      "Epoch: 260 Loss: [0.31736393]\n",
      "Epoch: 261 Loss: [0.31535476]\n",
      "Epoch: 262 Loss: [0.31336855]\n",
      "Epoch: 263 Loss: [0.31140451]\n",
      "Epoch: 264 Loss: [0.30946269]\n",
      "Epoch: 265 Loss: [0.30754235]\n",
      "Epoch: 266 Loss: [0.30564351]\n",
      "Epoch: 267 Loss: [0.30376547]\n",
      "Epoch: 268 Loss: [0.30190827]\n",
      "Epoch: 269 Loss: [0.30007121]\n",
      "Epoch: 270 Loss: [0.29825434]\n",
      "Epoch: 271 Loss: [0.29645699]\n",
      "Epoch: 272 Loss: [0.2946792]\n",
      "Epoch: 273 Loss: [0.29292035]\n",
      "Epoch: 274 Loss: [0.29118045]\n",
      "Epoch: 275 Loss: [0.28945891]\n",
      "Epoch: 276 Loss: [0.28775575]\n",
      "Epoch: 277 Loss: [0.2860704]\n",
      "Epoch: 278 Loss: [0.28440287]\n",
      "Epoch: 279 Loss: [0.28275263]\n",
      "Epoch: 280 Loss: [0.28111968]\n",
      "Epoch: 281 Loss: [0.27950349]\n",
      "Epoch: 282 Loss: [0.2779041]\n",
      "Epoch: 283 Loss: [0.27632098]\n",
      "Epoch: 284 Loss: [0.27475416]\n",
      "Epoch: 285 Loss: [0.27320313]\n",
      "Epoch: 286 Loss: [0.27166794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 287 Loss: [0.27014809]\n",
      "Epoch: 288 Loss: [0.26864361]\n",
      "Epoch: 289 Loss: [0.26715404]\n",
      "Epoch: 290 Loss: [0.2656794]\n",
      "Epoch: 291 Loss: [0.26421925]\n",
      "Epoch: 292 Loss: [0.26277362]\n",
      "Epoch: 293 Loss: [0.26134205]\n",
      "Epoch: 294 Loss: [0.2599246]\n",
      "Epoch: 295 Loss: [0.25852083]\n",
      "Epoch: 296 Loss: [0.25713079]\n",
      "Epoch: 297 Loss: [0.25575404]\n",
      "Epoch: 298 Loss: [0.25439064]\n",
      "Epoch: 299 Loss: [0.25304018]\n",
      "Epoch: 300 Loss: [0.2517027]\n",
      "Epoch: 301 Loss: [0.25037788]\n",
      "Epoch: 302 Loss: [0.24906541]\n",
      "Epoch: 303 Loss: [0.24776547]\n",
      "Epoch: 304 Loss: [0.24647789]\n",
      "Epoch: 305 Loss: [0.24520207]\n",
      "Epoch: 306 Loss: [0.24393798]\n",
      "Epoch: 307 Loss: [0.24268584]\n",
      "Epoch: 308 Loss: [0.2414454]\n",
      "Epoch: 309 Loss: [0.2402161]\n",
      "Epoch: 310 Loss: [0.2389979]\n",
      "Epoch: 311 Loss: [0.23779111]\n",
      "Epoch: 312 Loss: [0.23659517]\n",
      "Epoch: 313 Loss: [0.23541006]\n",
      "Epoch: 314 Loss: [0.23423553]\n",
      "Epoch: 315 Loss: [0.23307164]\n",
      "Epoch: 316 Loss: [0.23191807]\n",
      "Epoch: 317 Loss: [0.23077484]\n",
      "Epoch: 318 Loss: [0.22964166]\n",
      "Epoch: 319 Loss: [0.22851857]\n",
      "Epoch: 320 Loss: [0.22740527]\n",
      "Epoch: 321 Loss: [0.22630181]\n",
      "Epoch: 322 Loss: [0.22520797]\n",
      "Epoch: 323 Loss: [0.22412349]\n",
      "Epoch: 324 Loss: [0.22304854]\n",
      "Epoch: 325 Loss: [0.22198301]\n",
      "Epoch: 326 Loss: [0.22092646]\n",
      "Epoch: 327 Loss: [0.21987881]\n",
      "Epoch: 328 Loss: [0.21884036]\n",
      "Epoch: 329 Loss: [0.21781065]\n",
      "Epoch: 330 Loss: [0.21678968]\n",
      "Epoch: 331 Loss: [0.21577725]\n",
      "Epoch: 332 Loss: [0.21477342]\n",
      "Epoch: 333 Loss: [0.21377792]\n",
      "Epoch: 334 Loss: [0.2127908]\n",
      "Epoch: 335 Loss: [0.21181181]\n",
      "Epoch: 336 Loss: [0.21084101]\n",
      "Epoch: 337 Loss: [0.2098782]\n",
      "Epoch: 338 Loss: [0.20892318]\n",
      "Epoch: 339 Loss: [0.2079761]\n",
      "Epoch: 340 Loss: [0.20703689]\n",
      "Epoch: 341 Loss: [0.20610516]\n",
      "Epoch: 342 Loss: [0.20518085]\n",
      "Epoch: 343 Loss: [0.20426422]\n",
      "Epoch: 344 Loss: [0.20335492]\n",
      "Epoch: 345 Loss: [0.20245292]\n",
      "Epoch: 346 Loss: [0.20155806]\n",
      "Epoch: 347 Loss: [0.2006704]\n",
      "Epoch: 348 Loss: [0.19978979]\n",
      "Epoch: 349 Loss: [0.19891601]\n",
      "Epoch: 350 Loss: [0.19804922]\n",
      "Epoch: 351 Loss: [0.19718936]\n",
      "Epoch: 352 Loss: [0.1963361]\n",
      "Epoch: 353 Loss: [0.19548936]\n",
      "Epoch: 354 Loss: [0.1946494]\n",
      "Epoch: 355 Loss: [0.1938159]\n",
      "Epoch: 356 Loss: [0.19298885]\n",
      "Epoch: 357 Loss: [0.1921681]\n",
      "Epoch: 358 Loss: [0.1913537]\n",
      "Epoch: 359 Loss: [0.19054553]\n",
      "Epoch: 360 Loss: [0.1897434]\n",
      "Epoch: 361 Loss: [0.18894745]\n",
      "Epoch: 362 Loss: [0.18815764]\n",
      "Epoch: 363 Loss: [0.18737366]\n",
      "Epoch: 364 Loss: [0.18659546]\n",
      "Epoch: 365 Loss: [0.18582326]\n",
      "Epoch: 366 Loss: [0.18505678]\n",
      "Epoch: 367 Loss: [0.18429602]\n",
      "Epoch: 368 Loss: [0.18354085]\n",
      "Epoch: 369 Loss: [0.18279132]\n",
      "Epoch: 370 Loss: [0.18204731]\n",
      "Epoch: 371 Loss: [0.18130866]\n",
      "Epoch: 372 Loss: [0.18057554]\n",
      "Epoch: 373 Loss: [0.17984774]\n",
      "Epoch: 374 Loss: [0.17912524]\n",
      "Epoch: 375 Loss: [0.17840794]\n",
      "Epoch: 376 Loss: [0.17769587]\n",
      "Epoch: 377 Loss: [0.17698893]\n",
      "Epoch: 378 Loss: [0.17628698]\n",
      "Epoch: 379 Loss: [0.17559011]\n",
      "Epoch: 380 Loss: [0.17489833]\n",
      "Epoch: 381 Loss: [0.17421137]\n",
      "Epoch: 382 Loss: [0.17352918]\n",
      "Epoch: 383 Loss: [0.17285195]\n",
      "Epoch: 384 Loss: [0.17217946]\n",
      "Epoch: 385 Loss: [0.17151172]\n",
      "Epoch: 386 Loss: [0.17084865]\n",
      "Epoch: 387 Loss: [0.17019011]\n",
      "Epoch: 388 Loss: [0.16953623]\n",
      "Epoch: 389 Loss: [0.16888699]\n",
      "Epoch: 390 Loss: [0.16824215]\n",
      "Epoch: 391 Loss: [0.16760166]\n",
      "Epoch: 392 Loss: [0.1669657]\n",
      "Epoch: 393 Loss: [0.16633411]\n",
      "Epoch: 394 Loss: [0.16570674]\n",
      "Epoch: 395 Loss: [0.16508368]\n",
      "Epoch: 396 Loss: [0.16446496]\n",
      "Epoch: 397 Loss: [0.16385036]\n",
      "Epoch: 398 Loss: [0.16323981]\n",
      "Epoch: 399 Loss: [0.16263348]\n",
      "Epoch: 400 Loss: [0.1620312]\n",
      "Epoch: 401 Loss: [0.16143298]\n",
      "Epoch: 402 Loss: [0.16083874]\n",
      "Epoch: 403 Loss: [0.16024837]\n",
      "Epoch: 404 Loss: [0.15966201]\n",
      "Epoch: 405 Loss: [0.15907951]\n",
      "Epoch: 406 Loss: [0.15850086]\n",
      "Epoch: 407 Loss: [0.15792598]\n",
      "Epoch: 408 Loss: [0.15735491]\n",
      "Epoch: 409 Loss: [0.15678758]\n",
      "Epoch: 410 Loss: [0.15622386]\n",
      "Epoch: 411 Loss: [0.1556639]\n",
      "Epoch: 412 Loss: [0.15510754]\n",
      "Epoch: 413 Loss: [0.1545548]\n",
      "Epoch: 414 Loss: [0.15400561]\n",
      "Epoch: 415 Loss: [0.15345988]\n",
      "Epoch: 416 Loss: [0.1529177]\n",
      "Epoch: 417 Loss: [0.15237907]\n",
      "Epoch: 418 Loss: [0.15184381]\n",
      "Epoch: 419 Loss: [0.15131186]\n",
      "Epoch: 420 Loss: [0.15078337]\n",
      "Epoch: 421 Loss: [0.15025825]\n",
      "Epoch: 422 Loss: [0.14973635]\n",
      "Epoch: 423 Loss: [0.14921776]\n",
      "Epoch: 424 Loss: [0.14870251]\n",
      "Epoch: 425 Loss: [0.14819042]\n",
      "Epoch: 426 Loss: [0.14768147]\n",
      "Epoch: 427 Loss: [0.14717567]\n",
      "Epoch: 428 Loss: [0.14667307]\n",
      "Epoch: 429 Loss: [0.14617364]\n",
      "Epoch: 430 Loss: [0.14567722]\n",
      "Epoch: 431 Loss: [0.14518377]\n",
      "Epoch: 432 Loss: [0.14469344]\n",
      "Epoch: 433 Loss: [0.14420612]\n",
      "Epoch: 434 Loss: [0.1437217]\n",
      "Epoch: 435 Loss: [0.14324025]\n",
      "Epoch: 436 Loss: [0.1427618]\n",
      "Epoch: 437 Loss: [0.14228619]\n",
      "Epoch: 438 Loss: [0.14181337]\n",
      "Epoch: 439 Loss: [0.14134348]\n",
      "Epoch: 440 Loss: [0.14087643]\n",
      "Epoch: 441 Loss: [0.14041209]\n",
      "Epoch: 442 Loss: [0.13995059]\n",
      "Epoch: 443 Loss: [0.13949182]\n",
      "Epoch: 444 Loss: [0.13903578]\n",
      "Epoch: 445 Loss: [0.13858245]\n",
      "Epoch: 446 Loss: [0.13813173]\n",
      "Epoch: 447 Loss: [0.13768373]\n",
      "Epoch: 448 Loss: [0.13723835]\n",
      "Epoch: 449 Loss: [0.13679559]\n",
      "Epoch: 450 Loss: [0.13635539]\n",
      "Epoch: 451 Loss: [0.13591778]\n",
      "Epoch: 452 Loss: [0.13548272]\n",
      "Epoch: 453 Loss: [0.13505013]\n",
      "Epoch: 454 Loss: [0.13462009]\n",
      "Epoch: 455 Loss: [0.13419253]\n",
      "Epoch: 456 Loss: [0.13376744]\n",
      "Epoch: 457 Loss: [0.13334479]\n",
      "Epoch: 458 Loss: [0.1329245]\n",
      "Epoch: 459 Loss: [0.13250666]\n",
      "Epoch: 460 Loss: [0.1320912]\n",
      "Epoch: 461 Loss: [0.13167811]\n",
      "Epoch: 462 Loss: [0.13126736]\n",
      "Epoch: 463 Loss: [0.13085888]\n",
      "Epoch: 464 Loss: [0.13045276]\n",
      "Epoch: 465 Loss: [0.13004891]\n",
      "Epoch: 466 Loss: [0.12964734]\n",
      "Epoch: 467 Loss: [0.12924802]\n",
      "Epoch: 468 Loss: [0.12885087]\n",
      "Epoch: 469 Loss: [0.12845599]\n",
      "Epoch: 470 Loss: [0.12806329]\n",
      "Epoch: 471 Loss: [0.12767278]\n",
      "Epoch: 472 Loss: [0.12728443]\n",
      "Epoch: 473 Loss: [0.12689817]\n",
      "Epoch: 474 Loss: [0.12651409]\n",
      "Epoch: 475 Loss: [0.1261321]\n",
      "Epoch: 476 Loss: [0.12575222]\n",
      "Epoch: 477 Loss: [0.12537441]\n",
      "Epoch: 478 Loss: [0.12499861]\n",
      "Epoch: 479 Loss: [0.1246249]\n",
      "Epoch: 480 Loss: [0.12425321]\n",
      "Epoch: 481 Loss: [0.12388355]\n",
      "Epoch: 482 Loss: [0.12351587]\n",
      "Epoch: 483 Loss: [0.12315013]\n",
      "Epoch: 484 Loss: [0.1227864]\n",
      "Epoch: 485 Loss: [0.1224246]\n",
      "Epoch: 486 Loss: [0.12206476]\n",
      "Epoch: 487 Loss: [0.12170683]\n",
      "Epoch: 488 Loss: [0.12135076]\n",
      "Epoch: 489 Loss: [0.12099662]\n",
      "Epoch: 490 Loss: [0.12064435]\n",
      "Epoch: 491 Loss: [0.12029396]\n",
      "Epoch: 492 Loss: [0.11994541]\n",
      "Epoch: 493 Loss: [0.11959864]\n",
      "Epoch: 494 Loss: [0.11925374]\n",
      "Epoch: 495 Loss: [0.11891064]\n",
      "Epoch: 496 Loss: [0.11856933]\n",
      "Epoch: 497 Loss: [0.11822981]\n",
      "Epoch: 498 Loss: [0.11789201]\n",
      "Epoch: 499 Loss: [0.11755599]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 10\n",
    "num_batches = num_samples/batch_size\n",
    "alpha = 0.9\n",
    "eps = 1e-4\n",
    "loss1 = []\n",
    "v = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        \n",
    "        W1 = W1 + alpha*v[\"W1\"]\n",
    "        b1 = b1 + alpha*v[\"b1\"]\n",
    "        W2 = W2 + alpha*v[\"W2\"]\n",
    "        b2 = b2 + alpha*v[\"b2\"]\n",
    "        W3 = W3 + alpha*v[\"W3\"]\n",
    "        b3 = b3 + alpha*v[\"b3\"]\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j].reshape(-1,1).T)\n",
    "        \n",
    "        v[\"W1\"] = alpha*v[\"W1\"] - eps*(W1_upd)\n",
    "        v[\"W2\"] = alpha*v[\"W2\"] - eps*(W2_upd)\n",
    "        v[\"W3\"] = alpha*v[\"W3\"] - eps*(W3_upd)\n",
    "        v[\"b1\"] = alpha*v[\"b1\"] - eps*(b1_upd)\n",
    "        v[\"b2\"] = alpha*v[\"b2\"] - eps*(b2_upd)\n",
    "        v[\"b3\"] = alpha*v[\"b3\"] - eps*(b3_upd)\n",
    "        W3 = W3 + v[\"W3\"]\n",
    "        W2 = W2 + v[\"W2\"]\n",
    "        W1 = W1 + v[\"W1\"]\n",
    "        b3 = b3 + v[\"b3\"]\n",
    "        b2 = b2 + v[\"b2\"]\n",
    "        b1 = b1 + v[\"b1\"]\n",
    "    loss1.append(loss)\n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.64220864e-10]\n",
      " [1.57664409e-03]\n",
      " [9.98423356e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 1, 2, 1, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "acc =accuracy_score(y_pred=preds,y_true=true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = np.arange(1,501)\n",
    "plt.plot(epochs,loss1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.2283842968529,103.10987612278778,101.00093985479928,88.32669505969353,73.18538317707046,60.668400556533825,52.42852421492809,47.2364046317914,43.511314349223966,40.92984885029928,39.03442323894546,37.76094229255115,36.502380261349266,35.53039101931907,34.661701211043244,33.838107417725986,33.04749423047696,32.32842438843492,31.646236728123608,30.951736464930185,30.287958792396545,29.621784509042016,28.977806833487893,28.34059338262265,27.721647347616084,27.12366513209173,26.558043614882386,26.010858944150147,25.512178026495437,25.02941212134341,24.59099766479524,24.16816086342705,23.79035730212456,23.434649472317812,23.11206230346273,22.812845969747098,22.53623047386306,22.277462937330263,22.03215638065645,21.796690694933254,21.566979552611627,21.33980885939497,21.112232302357512,20.881612471026617,20.645792413751607,20.40294917048321,20.15152030044,19.890178069991475,19.618366920629125,19.3303901437666,19.03891561892443,18.728183583496406,18.402780517142894,18.062489825689358,17.704105367692154,17.327028064145743,16.92950946549262,16.511429069251086,16.07003420544299,15.603348570828038,15.109895338864492,14.588198189551656,14.03721793440561,13.457422764335623,12.850140582407185,12.219230932525342,11.571815240793898,10.918108230671722,10.270845967662769,9.643272719564266,9.046853145641485,8.489023246819679,7.972728333004739,7.497724996605187,7.062065230371151,6.662986401585195,6.297379971461554,5.962119905123053,5.654250715705108,5.371015580661733,5.1100662350998745,4.86904324138491,4.646046398613526,4.439343225265194,4.247357639920354,4.068682001448408,3.902209032189795,3.7466714856912344,3.6012482967543185,3.464977835497353,3.3371512763597577,3.2170166881382327,3.1039945252914856,2.9974790026401887,2.8970015491875243,2.8020639263574325,2.7123008532753508,2.6272497602475453,2.546684399626922,2.470177929942298,2.3975359169434944,2.3284298027204486,2.2626746902923522,2.2000035441984243,2.140281508596207,2.0832262948645677,2.0287898055670133,1.976689559181518,1.9269009959532102,1.8791717920774906,1.833501850587284,1.7896404883165395,1.74764337915584,1.7072159947240577,1.66849158979778,1.6311344390552407,1.595334146834691,1.5607374042376985,1.5275595115825296,1.4954365971832027,1.4646302019910888,1.434737554196546,1.4060656424240698,1.3781979440204786,1.351451334051003,1.3254302922255001,1.3004276018700112,1.2760856353611476,1.2526661587101153,1.2298625568179138,1.207886981882054,1.1864850997229033,1.1658307197943865,1.145715707218851,1.1262702417733963,1.1073368534426296,1.089002312521771,1.0711546392020401,1.0538446880826382,1.0369938463685768,1.0206304075076584,1.0047110105026276,0.9892265444412415,0.9741433688880146,0.9594750691412893,0.9451780100359347,0.931261431226415,0.9176934401186074,0.9044773144868195,0.891587420609239,0.8790195485052699,0.8667622962103301,0.8548006774261572,0.8431298385313268,0.8317340629336932,0.8206114985487861,0.8097459999056967,0.7991322080528698,0.7887641489274893,0.7786293380217182,0.7687224131885336,0.7590385423482442,0.7495664376580601,0.7403012541939405,0.73123916577133,0.7223697787431151,0.7136887990883268,0.7051931934411592,0.6968733995955899,0.6887255640300998,0.6807473361561847,0.6729299078905472,0.6652697778216295,0.657765186903263,0.6504080114685609,0.643195032430056,0.6361250014270683,0.629190419723319,0.6223882942106956,0.6157178158330475,0.6091720594535533,0.6027482126788887,0.5964458427266564,0.5902585531024387,0.5841836761547294,0.5782210996852452,0.5723649158957348,0.5666125736493403,0.5609642321724115,0.5554144363718706,0.5499607297985207,0.5446038044121652,0.5393365953507604,0.5341625193944639,0.5290735899159567,0.5240709262465302,0.5191533275695306,0.5143164465348585,0.5095589425863036,0.5048802047377582,0.5002776909725499,0.4957484565491535,0.49129219249296713,0.48690776544209613,0.4825924088561401,0.47834384477464087,0.47416268069274536,0.47004655165174836,0.4659927526830705,0.4620012341472459,0.4580712093254309,0.4542003078872765,0.45038651482341396,0.4466307285338951,0.44292989862220344,0.4392837530248367,0.43569141872275474,0.4321504577489213,0.4286606689946474,0.42522174479837854,0.4218317703791629,0.41848894440451345,0.415194173674225,0.41194506189786423,0.40874149950479116,0.4055819109599666,0.4024662052797859,0.3993927027577453,0.39636118715015023,0.3933710189483301,0.3904203711627945,0.38750909196973304,0.3846371298645296,0.3818030876201346,0.37900546393053414,0.37624507191113,0.3735202168936649,0.37083075725338877,0.3681755360533581,0.36555454431862244,0.36296652754682757,0.3604114740472723,0.35788823739981535,0.3553966417146358,0.3529363288830183,0.3505059138382091,0.34810525688609933,0.3457344810122233,0.3433926100267255,0.34107853099385027,0.33879225756551795,0.33653385784823,0.33430241651726045,0.3320967758258016,0.3299175930679776,0.32776381226176654,0.32563531103552934,0.32353129020208865,0.32145180764325315,0.31939601346245106,0.31736392671718294,0.3153547627598421,0.3133685451389174,0.3114045135795702,0.3094626899894741,0.30754234801193764,0.3056435086174456,0.30376547424278594,0.3019082655595623,0.30007121287896066,0.29825433672910845,0.29645699340320875,0.2946792036609957,0.29292034825863206,0.2911804484498612,0.28945890796021495,0.28775574881256716,0.2860703963111124,0.284402873495392,0.2827526259350226,0.2811196779203124,0.279503494049871,0.2779041000823921,0.27632097848017245,0.2747541566735699,0.2732031338899597,0.27166793942082335,0.27014808822266356,0.2686436116246306,0.2671540393350522,0.26567940488513964,0.2642192518129175,0.26277361600451454,0.26134205395732396,0.2599246040544786,0.2585208349310205,0.25713078759899705,0.2557540420551516,0.25439064206272316,0.2530401782472184,0.251702697236337,0.2503778811281459,0.24906540613514477,0.24776547438867416,0.24647788576832594,0.24520207329437319,0.24393798128606073,0.24268583846025876,0.24144539542351648,0.24021609506931643,0.238997897866727,0.2377911129700516,0.23659516938909972,0.23541005665345685,0.23423553147008974,0.2330716422513914,0.23191806847938493,0.2307748387341617,0.22964165807969047,0.22851856772136095,0.2274052725740488,0.22630181490631915,0.22520797264783077,0.22412348994097933,0.22304854117644016,0.22198301223898215,0.22092645748566941,0.2198788133147183,0.218840356349421,0.21781065114889642,0.21678968231968102,0.21577725368627998,0.21477341654143117,0.21377792189732822,0.21279079927723715,0.21181181276555375,0.21084100536436556,0.209878201408494,0.20892318092462653,0.20797610050094795,0.20703688639684242,0.20610515963527704,0.20518085413947051,0.2042642245279971,0.2033549172069895,0.20245291987042263,0.20155806319017952,0.20067040223149654,0.19978978514921453,0.1989160085847683,0.19804921590171354,0.1971893568373781,0.19633609507426442,0.19548936362463235,0.1946493985220856,0.1938158970035542,0.19298884679613967,0.19216809745517557,0.19135370185302825,0.19054553184260337,0.18974340402101786,0.1889474503983186,0.1881576374002013,0.18737366389221846,0.18659546332470187,0.18582325644563838,0.18505677952807664,0.1842960240291479,0.1835408524922244,0.1827913180979953,0.18204730984302947,0.18130866181756528,0.18057554305755286,0.17984773608943785,0.17912524332319044,0.1784079406863468,0.17769586986844904,0.17698893316033998,0.17628697553074116,0.17559011222732185,0.17489832929412719,0.17421137427486483,0.17352918249299942,0.1728519496580795,0.1721794648459629,0.17151172301113507,0.1708486485403371,0.1701901146804907,0.16953623222777772,0.16888699046672356,0.16824215454096364,0.1676016616365063,0.16696569836218264,0.16633411246755986,0.1657067368575322,0.16508368379793684,0.16446496349030515,0.1638503595934916,0.1632398069667209,0.162633480496739,0.16203120485166844,0.1614329791353802,0.16083873785444652,0.16024836885107382,0.15966201069537714,0.15907950774067872,0.15850086325231774,0.15792598496884264,0.15735491151111491,0.15678757750847863,0.15622386367027533,0.15566389922852772,0.15510754197962,0.15455479741796518,0.154005613011715,0.1534598838781714,0.15291770038652308,0.15237906914540297,0.1518438080566284,0.15131185873278127,0.15078337493339886,0.15025825022001066,0.1497363500693448,0.1492177632679147,0.1487025097029432,0.14819042078213268,0.1476814698067864,0.1471756729434485,0.14667306591471235,0.1461736351223529,0.14567722016911222,0.14518377301869753,0.1446934369303376,0.14420611953991103,0.14372169661557413,0.14324024822832104,0.14276179678282233,0.1422861905504835,0.14181337370465324,0.1413434809857237,0.14087642639614392,0.14041209330268983,0.13995058864800358,0.13949181532905855,0.139035781440443,0.13858244822021368,0.13813173004255433,0.13768372957816272,0.13723834862246276,0.13679559015912177,0.13635539169865477,0.13591778224121018,0.13548272339422915,0.1350501281256497,0.13462009487367319,0.13419252892763903,0.1337674375309194,0.13334478583612422,0.13292449560475023,0.13250666402721278,0.1320912011418641,0.1316781122206462,0.1312673645064801,0.1308588822878766,0.13045275941031986,0.13004891076401826,0.12964734086247634,0.12924801901105445,0.12885087177154692,0.1284559897962055,0.12806329232393335,0.12767278337740812,0.1272844340784593,0.12689817316322216,0.12651408835764225,0.12613210280355297,0.12575222025190022,0.1253744133969892,0.12499861305553411,0.12462490428078755,0.12425321370786192,0.12388354501618253,0.12351587224685742,0.12315012820874707,0.1227863955190507,0.12242460392779697,0.12206475722581943,0.12170683059298108,0.12135075874702463,0.12099662208991904,0.12064435313685819,0.1202939559572783,0.11994540667785078,0.11959864184678332,0.11925373985688159,0.1189106356652616,0.11856933377248041,0.11822981107511965,0.11789200587779888,0.11755599475653217,"
     ]
    }
   ],
   "source": [
    "for elem in np.array(loss1):\n",
    "    print(str(elem[0])+',' , end = '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
