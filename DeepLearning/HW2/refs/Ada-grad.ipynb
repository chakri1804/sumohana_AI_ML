{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grumptitan/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [2.04852162]\n",
      "Epoch: 1 Loss: [1.92808324]\n",
      "Epoch: 2 Loss: [1.85572404]\n",
      "Epoch: 3 Loss: [1.79959588]\n",
      "Epoch: 4 Loss: [1.75094407]\n",
      "Epoch: 5 Loss: [1.70230433]\n",
      "Epoch: 6 Loss: [1.65010259]\n",
      "Epoch: 7 Loss: [1.60193355]\n",
      "Epoch: 8 Loss: [1.55711668]\n",
      "Epoch: 9 Loss: [1.5146096]\n",
      "Epoch: 10 Loss: [1.47490392]\n",
      "Epoch: 11 Loss: [1.43900804]\n",
      "Epoch: 12 Loss: [1.40699622]\n",
      "Epoch: 13 Loss: [1.37753109]\n",
      "Epoch: 14 Loss: [1.34956381]\n",
      "Epoch: 15 Loss: [1.32274108]\n",
      "Epoch: 16 Loss: [1.29692056]\n",
      "Epoch: 17 Loss: [1.27197607]\n",
      "Epoch: 18 Loss: [1.24789286]\n",
      "Epoch: 19 Loss: [1.22457575]\n",
      "Epoch: 20 Loss: [1.20204797]\n",
      "Epoch: 21 Loss: [1.18019338]\n",
      "Epoch: 22 Loss: [1.15901819]\n",
      "Epoch: 23 Loss: [1.13850775]\n",
      "Epoch: 24 Loss: [1.11868594]\n",
      "Epoch: 25 Loss: [1.0997071]\n",
      "Epoch: 26 Loss: [1.08146543]\n",
      "Epoch: 27 Loss: [1.06382312]\n",
      "Epoch: 28 Loss: [1.04677637]\n",
      "Epoch: 29 Loss: [1.03036659]\n",
      "Epoch: 30 Loss: [1.01446571]\n",
      "Epoch: 31 Loss: [0.99905998]\n",
      "Epoch: 32 Loss: [0.98413401]\n",
      "Epoch: 33 Loss: [0.96962556]\n",
      "Epoch: 34 Loss: [0.95554027]\n",
      "Epoch: 35 Loss: [0.94180802]\n",
      "Epoch: 36 Loss: [0.92839389]\n",
      "Epoch: 37 Loss: [0.91537028]\n",
      "Epoch: 38 Loss: [0.9027077]\n",
      "Epoch: 39 Loss: [0.89035417]\n",
      "Epoch: 40 Loss: [0.87832337]\n",
      "Epoch: 41 Loss: [0.86658897]\n",
      "Epoch: 42 Loss: [0.85516845]\n",
      "Epoch: 43 Loss: [0.84403879]\n",
      "Epoch: 44 Loss: [0.83318364]\n",
      "Epoch: 45 Loss: [0.82254649]\n",
      "Epoch: 46 Loss: [0.81214874]\n",
      "Epoch: 47 Loss: [0.8018898]\n",
      "Epoch: 48 Loss: [0.79166721]\n",
      "Epoch: 49 Loss: [0.78151716]\n",
      "Epoch: 50 Loss: [0.77153817]\n",
      "Epoch: 51 Loss: [0.76170276]\n",
      "Epoch: 52 Loss: [0.75210092]\n",
      "Epoch: 53 Loss: [0.74276862]\n",
      "Epoch: 54 Loss: [0.73363048]\n",
      "Epoch: 55 Loss: [0.72473815]\n",
      "Epoch: 56 Loss: [0.71594262]\n",
      "Epoch: 57 Loss: [0.70708171]\n",
      "Epoch: 58 Loss: [0.69829021]\n",
      "Epoch: 59 Loss: [0.68947795]\n",
      "Epoch: 60 Loss: [0.68066618]\n",
      "Epoch: 61 Loss: [0.67210282]\n",
      "Epoch: 62 Loss: [0.66388879]\n",
      "Epoch: 63 Loss: [0.65612451]\n",
      "Epoch: 64 Loss: [0.64871844]\n",
      "Epoch: 65 Loss: [0.64164568]\n",
      "Epoch: 66 Loss: [0.63479869]\n",
      "Epoch: 67 Loss: [0.628215]\n",
      "Epoch: 68 Loss: [0.6218853]\n",
      "Epoch: 69 Loss: [0.61577395]\n",
      "Epoch: 70 Loss: [0.60986835]\n",
      "Epoch: 71 Loss: [0.60415599]\n",
      "Epoch: 72 Loss: [0.59862513]\n",
      "Epoch: 73 Loss: [0.59325451]\n",
      "Epoch: 74 Loss: [0.58801157]\n",
      "Epoch: 75 Loss: [0.58293815]\n",
      "Epoch: 76 Loss: [0.57804101]\n",
      "Epoch: 77 Loss: [0.57328845]\n",
      "Epoch: 78 Loss: [0.56870514]\n",
      "Epoch: 79 Loss: [0.56427562]\n",
      "Epoch: 80 Loss: [0.5599841]\n",
      "Epoch: 81 Loss: [0.55582542]\n",
      "Epoch: 82 Loss: [0.55178275]\n",
      "Epoch: 83 Loss: [0.54788427]\n",
      "Epoch: 84 Loss: [0.54410391]\n",
      "Epoch: 85 Loss: [0.54043304]\n",
      "Epoch: 86 Loss: [0.53688401]\n",
      "Epoch: 87 Loss: [0.53344822]\n",
      "Epoch: 88 Loss: [0.5301087]\n",
      "Epoch: 89 Loss: [0.52689716]\n",
      "Epoch: 90 Loss: [0.5238066]\n",
      "Epoch: 91 Loss: [0.52080076]\n",
      "Epoch: 92 Loss: [0.51788329]\n",
      "Epoch: 93 Loss: [0.51505096]\n",
      "Epoch: 94 Loss: [0.51229133]\n",
      "Epoch: 95 Loss: [0.50960559]\n",
      "Epoch: 96 Loss: [0.50698567]\n",
      "Epoch: 97 Loss: [0.50443024]\n",
      "Epoch: 98 Loss: [0.50193688]\n",
      "Epoch: 99 Loss: [0.49950391]\n",
      "Epoch: 100 Loss: [0.49712859]\n",
      "Epoch: 101 Loss: [0.49481173]\n",
      "Epoch: 102 Loss: [0.49254777]\n",
      "Epoch: 103 Loss: [0.49034465]\n",
      "Epoch: 104 Loss: [0.48819978]\n",
      "Epoch: 105 Loss: [0.48610597]\n",
      "Epoch: 106 Loss: [0.48405381]\n",
      "Epoch: 107 Loss: [0.48204188]\n",
      "Epoch: 108 Loss: [0.48006771]\n",
      "Epoch: 109 Loss: [0.47815721]\n",
      "Epoch: 110 Loss: [0.47628677]\n",
      "Epoch: 111 Loss: [0.47445452]\n",
      "Epoch: 112 Loss: [0.47265792]\n",
      "Epoch: 113 Loss: [0.47090705]\n",
      "Epoch: 114 Loss: [0.46919727]\n",
      "Epoch: 115 Loss: [0.46752785]\n",
      "Epoch: 116 Loss: [0.46590301]\n",
      "Epoch: 117 Loss: [0.46431173]\n",
      "Epoch: 118 Loss: [0.46274898]\n",
      "Epoch: 119 Loss: [0.46121195]\n",
      "Epoch: 120 Loss: [0.45971018]\n",
      "Epoch: 121 Loss: [0.45824561]\n",
      "Epoch: 122 Loss: [0.45680409]\n",
      "Epoch: 123 Loss: [0.45538826]\n",
      "Epoch: 124 Loss: [0.45399719]\n",
      "Epoch: 125 Loss: [0.45262816]\n",
      "Epoch: 126 Loss: [0.45129435]\n",
      "Epoch: 127 Loss: [0.44999417]\n",
      "Epoch: 128 Loss: [0.44871344]\n",
      "Epoch: 129 Loss: [0.4474533]\n",
      "Epoch: 130 Loss: [0.44621316]\n",
      "Epoch: 131 Loss: [0.44499308]\n",
      "Epoch: 132 Loss: [0.44378993]\n",
      "Epoch: 133 Loss: [0.44260503]\n",
      "Epoch: 134 Loss: [0.44143954]\n",
      "Epoch: 135 Loss: [0.44029626]\n",
      "Epoch: 136 Loss: [0.43917555]\n",
      "Epoch: 137 Loss: [0.4380716]\n",
      "Epoch: 138 Loss: [0.43698208]\n",
      "Epoch: 139 Loss: [0.43592592]\n",
      "Epoch: 140 Loss: [0.43489229]\n",
      "Epoch: 141 Loss: [0.43387846]\n",
      "Epoch: 142 Loss: [0.43289915]\n",
      "Epoch: 143 Loss: [0.43193324]\n",
      "Epoch: 144 Loss: [0.43098088]\n",
      "Epoch: 145 Loss: [0.43004426]\n",
      "Epoch: 146 Loss: [0.42912086]\n",
      "Epoch: 147 Loss: [0.42820971]\n",
      "Epoch: 148 Loss: [0.42730939]\n",
      "Epoch: 149 Loss: [0.42641958]\n",
      "Epoch: 150 Loss: [0.42554016]\n",
      "Epoch: 151 Loss: [0.42467836]\n",
      "Epoch: 152 Loss: [0.42382653]\n",
      "Epoch: 153 Loss: [0.42298927]\n",
      "Epoch: 154 Loss: [0.42216339]\n",
      "Epoch: 155 Loss: [0.42134662]\n",
      "Epoch: 156 Loss: [0.4205384]\n",
      "Epoch: 157 Loss: [0.41974215]\n",
      "Epoch: 158 Loss: [0.41895455]\n",
      "Epoch: 159 Loss: [0.41817571]\n",
      "Epoch: 160 Loss: [0.41740594]\n",
      "Epoch: 161 Loss: [0.41665414]\n",
      "Epoch: 162 Loss: [0.41591477]\n",
      "Epoch: 163 Loss: [0.41518909]\n",
      "Epoch: 164 Loss: [0.4144694]\n",
      "Epoch: 165 Loss: [0.41375746]\n",
      "Epoch: 166 Loss: [0.41305301]\n",
      "Epoch: 167 Loss: [0.41235584]\n",
      "Epoch: 168 Loss: [0.41166578]\n",
      "Epoch: 169 Loss: [0.41098265]\n",
      "Epoch: 170 Loss: [0.41030635]\n",
      "Epoch: 171 Loss: [0.40963712]\n",
      "Epoch: 172 Loss: [0.40897502]\n",
      "Epoch: 173 Loss: [0.40831919]\n",
      "Epoch: 174 Loss: [0.40766737]\n",
      "Epoch: 175 Loss: [0.40701974]\n",
      "Epoch: 176 Loss: [0.4063849]\n",
      "Epoch: 177 Loss: [0.40575571]\n",
      "Epoch: 178 Loss: [0.4051324]\n",
      "Epoch: 179 Loss: [0.40451484]\n",
      "Epoch: 180 Loss: [0.40390288]\n",
      "Epoch: 181 Loss: [0.40329639]\n",
      "Epoch: 182 Loss: [0.40269524]\n",
      "Epoch: 183 Loss: [0.40209934]\n",
      "Epoch: 184 Loss: [0.40151121]\n",
      "Epoch: 185 Loss: [0.40093518]\n",
      "Epoch: 186 Loss: [0.40036308]\n",
      "Epoch: 187 Loss: [0.39979625]\n",
      "Epoch: 188 Loss: [0.39923474]\n",
      "Epoch: 189 Loss: [0.39867791]\n",
      "Epoch: 190 Loss: [0.39812566]\n",
      "Epoch: 191 Loss: [0.39758029]\n",
      "Epoch: 192 Loss: [0.39704347]\n",
      "Epoch: 193 Loss: [0.3965094]\n",
      "Epoch: 194 Loss: [0.39598281]\n",
      "Epoch: 195 Loss: [0.39546073]\n",
      "Epoch: 196 Loss: [0.39494305]\n",
      "Epoch: 197 Loss: [0.39442912]\n",
      "Epoch: 198 Loss: [0.39392111]\n",
      "Epoch: 199 Loss: [0.39341742]\n",
      "Epoch: 200 Loss: [0.39291797]\n",
      "Epoch: 201 Loss: [0.39242264]\n",
      "Epoch: 202 Loss: [0.39193135]\n",
      "Epoch: 203 Loss: [0.39144858]\n",
      "Epoch: 204 Loss: [0.39096958]\n",
      "Epoch: 205 Loss: [0.39049473]\n",
      "Epoch: 206 Loss: [0.39002366]\n",
      "Epoch: 207 Loss: [0.38955849]\n",
      "Epoch: 208 Loss: [0.38909808]\n",
      "Epoch: 209 Loss: [0.38864174]\n",
      "Epoch: 210 Loss: [0.38818889]\n",
      "Epoch: 211 Loss: [0.38773946]\n",
      "Epoch: 212 Loss: [0.38729519]\n",
      "Epoch: 213 Loss: [0.38685419]\n",
      "Epoch: 214 Loss: [0.38641656]\n",
      "Epoch: 215 Loss: [0.38598245]\n",
      "Epoch: 216 Loss: [0.38555132]\n",
      "Epoch: 217 Loss: [0.38512328]\n",
      "Epoch: 218 Loss: [0.38469809]\n",
      "Epoch: 219 Loss: [0.38427577]\n",
      "Epoch: 220 Loss: [0.38385658]\n",
      "Epoch: 221 Loss: [0.38344042]\n",
      "Epoch: 222 Loss: [0.38302697]\n",
      "Epoch: 223 Loss: [0.38261637]\n",
      "Epoch: 224 Loss: [0.38220815]\n",
      "Epoch: 225 Loss: [0.38180286]\n",
      "Epoch: 226 Loss: [0.38140005]\n",
      "Epoch: 227 Loss: [0.38100011]\n",
      "Epoch: 228 Loss: [0.38060235]\n",
      "Epoch: 229 Loss: [0.38020867]\n",
      "Epoch: 230 Loss: [0.37981784]\n",
      "Epoch: 231 Loss: [0.37942939]\n",
      "Epoch: 232 Loss: [0.37904039]\n",
      "Epoch: 233 Loss: [0.37865291]\n",
      "Epoch: 234 Loss: [0.37826788]\n",
      "Epoch: 235 Loss: [0.37788495]\n",
      "Epoch: 236 Loss: [0.37750435]\n",
      "Epoch: 237 Loss: [0.37712597]\n",
      "Epoch: 238 Loss: [0.37674978]\n",
      "Epoch: 239 Loss: [0.37637574]\n",
      "Epoch: 240 Loss: [0.37600393]\n",
      "Epoch: 241 Loss: [0.37563398]\n",
      "Epoch: 242 Loss: [0.37526618]\n",
      "Epoch: 243 Loss: [0.3749004]\n",
      "Epoch: 244 Loss: [0.37453676]\n",
      "Epoch: 245 Loss: [0.3741753]\n",
      "Epoch: 246 Loss: [0.3738155]\n",
      "Epoch: 247 Loss: [0.37345782]\n",
      "Epoch: 248 Loss: [0.37310175]\n",
      "Epoch: 249 Loss: [0.37274774]\n",
      "Epoch: 250 Loss: [0.37239906]\n",
      "Epoch: 251 Loss: [0.37205065]\n",
      "Epoch: 252 Loss: [0.37170378]\n",
      "Epoch: 253 Loss: [0.37135935]\n",
      "Epoch: 254 Loss: [0.37101638]\n",
      "Epoch: 255 Loss: [0.37067498]\n",
      "Epoch: 256 Loss: [0.37033539]\n",
      "Epoch: 257 Loss: [0.36999738]\n",
      "Epoch: 258 Loss: [0.36966102]\n",
      "Epoch: 259 Loss: [0.36932614]\n",
      "Epoch: 260 Loss: [0.3689928]\n",
      "Epoch: 261 Loss: [0.36866149]\n",
      "Epoch: 262 Loss: [0.36833161]\n",
      "Epoch: 263 Loss: [0.36800332]\n",
      "Epoch: 264 Loss: [0.36767896]\n",
      "Epoch: 265 Loss: [0.36735663]\n",
      "Epoch: 266 Loss: [0.3670359]\n",
      "Epoch: 267 Loss: [0.36671627]\n",
      "Epoch: 268 Loss: [0.36639788]\n",
      "Epoch: 269 Loss: [0.36608104]\n",
      "Epoch: 270 Loss: [0.36576516]\n",
      "Epoch: 271 Loss: [0.36545062]\n",
      "Epoch: 272 Loss: [0.36513734]\n",
      "Epoch: 273 Loss: [0.36482402]\n",
      "Epoch: 274 Loss: [0.36450839]\n",
      "Epoch: 275 Loss: [0.36419347]\n",
      "Epoch: 276 Loss: [0.36388268]\n",
      "Epoch: 277 Loss: [0.36357334]\n",
      "Epoch: 278 Loss: [0.36326463]\n",
      "Epoch: 279 Loss: [0.3629546]\n",
      "Epoch: 280 Loss: [0.36264565]\n",
      "Epoch: 281 Loss: [0.3623379]\n",
      "Epoch: 282 Loss: [0.36203116]\n",
      "Epoch: 283 Loss: [0.36172202]\n",
      "Epoch: 284 Loss: [0.36140526]\n",
      "Epoch: 285 Loss: [0.3610873]\n",
      "Epoch: 286 Loss: [0.36078025]\n",
      "Epoch: 287 Loss: [0.36047884]\n",
      "Epoch: 288 Loss: [0.36017575]\n",
      "Epoch: 289 Loss: [0.3598776]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 Loss: [0.35957884]\n",
      "Epoch: 291 Loss: [0.35928312]\n",
      "Epoch: 292 Loss: [0.35898235]\n",
      "Epoch: 293 Loss: [0.35868549]\n",
      "Epoch: 294 Loss: [0.3583657]\n",
      "Epoch: 295 Loss: [0.35805072]\n",
      "Epoch: 296 Loss: [0.35774613]\n",
      "Epoch: 297 Loss: [0.35744353]\n",
      "Epoch: 298 Loss: [0.35713733]\n",
      "Epoch: 299 Loss: [0.35683177]\n",
      "Epoch: 300 Loss: [0.35652616]\n",
      "Epoch: 301 Loss: [0.356226]\n",
      "Epoch: 302 Loss: [0.35592631]\n",
      "Epoch: 303 Loss: [0.35562772]\n",
      "Epoch: 304 Loss: [0.35531819]\n",
      "Epoch: 305 Loss: [0.35501375]\n",
      "Epoch: 306 Loss: [0.35470752]\n",
      "Epoch: 307 Loss: [0.35440159]\n",
      "Epoch: 308 Loss: [0.3540969]\n",
      "Epoch: 309 Loss: [0.35379864]\n",
      "Epoch: 310 Loss: [0.35349532]\n",
      "Epoch: 311 Loss: [0.35319516]\n",
      "Epoch: 312 Loss: [0.35289086]\n",
      "Epoch: 313 Loss: [0.35258486]\n",
      "Epoch: 314 Loss: [0.35227917]\n",
      "Epoch: 315 Loss: [0.35197425]\n",
      "Epoch: 316 Loss: [0.35166768]\n",
      "Epoch: 317 Loss: [0.35135952]\n",
      "Epoch: 318 Loss: [0.35105113]\n",
      "Epoch: 319 Loss: [0.35074469]\n",
      "Epoch: 320 Loss: [0.35043354]\n",
      "Epoch: 321 Loss: [0.35012177]\n",
      "Epoch: 322 Loss: [0.3498065]\n",
      "Epoch: 323 Loss: [0.34949182]\n",
      "Epoch: 324 Loss: [0.34917963]\n",
      "Epoch: 325 Loss: [0.3488647]\n",
      "Epoch: 326 Loss: [0.34855045]\n",
      "Epoch: 327 Loss: [0.34823609]\n",
      "Epoch: 328 Loss: [0.34792175]\n",
      "Epoch: 329 Loss: [0.34758758]\n",
      "Epoch: 330 Loss: [0.34724592]\n",
      "Epoch: 331 Loss: [0.34691309]\n",
      "Epoch: 332 Loss: [0.34656904]\n",
      "Epoch: 333 Loss: [0.34621846]\n",
      "Epoch: 334 Loss: [0.34586397]\n",
      "Epoch: 335 Loss: [0.34549853]\n",
      "Epoch: 336 Loss: [0.34512684]\n",
      "Epoch: 337 Loss: [0.34474221]\n",
      "Epoch: 338 Loss: [0.34435717]\n",
      "Epoch: 339 Loss: [0.3439846]\n",
      "Epoch: 340 Loss: [0.34362725]\n",
      "Epoch: 341 Loss: [0.34327211]\n",
      "Epoch: 342 Loss: [0.3429195]\n",
      "Epoch: 343 Loss: [0.34256824]\n",
      "Epoch: 344 Loss: [0.34221812]\n",
      "Epoch: 345 Loss: [0.34187066]\n",
      "Epoch: 346 Loss: [0.34152912]\n",
      "Epoch: 347 Loss: [0.34118555]\n",
      "Epoch: 348 Loss: [0.34084044]\n",
      "Epoch: 349 Loss: [0.34050119]\n",
      "Epoch: 350 Loss: [0.3401658]\n",
      "Epoch: 351 Loss: [0.33983387]\n",
      "Epoch: 352 Loss: [0.33950738]\n",
      "Epoch: 353 Loss: [0.33918675]\n",
      "Epoch: 354 Loss: [0.3388698]\n",
      "Epoch: 355 Loss: [0.33855504]\n",
      "Epoch: 356 Loss: [0.33824223]\n",
      "Epoch: 357 Loss: [0.33792985]\n",
      "Epoch: 358 Loss: [0.3376179]\n",
      "Epoch: 359 Loss: [0.33730843]\n",
      "Epoch: 360 Loss: [0.33700088]\n",
      "Epoch: 361 Loss: [0.33669393]\n",
      "Epoch: 362 Loss: [0.33638959]\n",
      "Epoch: 363 Loss: [0.33608855]\n",
      "Epoch: 364 Loss: [0.33579046]\n",
      "Epoch: 365 Loss: [0.33549286]\n",
      "Epoch: 366 Loss: [0.33519618]\n",
      "Epoch: 367 Loss: [0.33489995]\n",
      "Epoch: 368 Loss: [0.33460424]\n",
      "Epoch: 369 Loss: [0.33430981]\n",
      "Epoch: 370 Loss: [0.33401889]\n",
      "Epoch: 371 Loss: [0.33372922]\n",
      "Epoch: 372 Loss: [0.33344069]\n",
      "Epoch: 373 Loss: [0.33315277]\n",
      "Epoch: 374 Loss: [0.3328681]\n",
      "Epoch: 375 Loss: [0.33258461]\n",
      "Epoch: 376 Loss: [0.33230182]\n",
      "Epoch: 377 Loss: [0.3320197]\n",
      "Epoch: 378 Loss: [0.33173803]\n",
      "Epoch: 379 Loss: [0.33145745]\n",
      "Epoch: 380 Loss: [0.33117897]\n",
      "Epoch: 381 Loss: [0.3309027]\n",
      "Epoch: 382 Loss: [0.33062758]\n",
      "Epoch: 383 Loss: [0.33035418]\n",
      "Epoch: 384 Loss: [0.33008249]\n",
      "Epoch: 385 Loss: [0.32981247]\n",
      "Epoch: 386 Loss: [0.32954525]\n",
      "Epoch: 387 Loss: [0.32927942]\n",
      "Epoch: 388 Loss: [0.32901478]\n",
      "Epoch: 389 Loss: [0.32875102]\n",
      "Epoch: 390 Loss: [0.32848816]\n",
      "Epoch: 391 Loss: [0.32822627]\n",
      "Epoch: 392 Loss: [0.32796515]\n",
      "Epoch: 393 Loss: [0.32770535]\n",
      "Epoch: 394 Loss: [0.32744626]\n",
      "Epoch: 395 Loss: [0.32718846]\n",
      "Epoch: 396 Loss: [0.32693114]\n",
      "Epoch: 397 Loss: [0.32667457]\n",
      "Epoch: 398 Loss: [0.32641874]\n",
      "Epoch: 399 Loss: [0.32616374]\n",
      "Epoch: 400 Loss: [0.32590927]\n",
      "Epoch: 401 Loss: [0.32565602]\n",
      "Epoch: 402 Loss: [0.32540379]\n",
      "Epoch: 403 Loss: [0.32515299]\n",
      "Epoch: 404 Loss: [0.32490293]\n",
      "Epoch: 405 Loss: [0.3246534]\n",
      "Epoch: 406 Loss: [0.32440567]\n",
      "Epoch: 407 Loss: [0.32415968]\n",
      "Epoch: 408 Loss: [0.32391472]\n",
      "Epoch: 409 Loss: [0.32367101]\n",
      "Epoch: 410 Loss: [0.32342734]\n",
      "Epoch: 411 Loss: [0.3231854]\n",
      "Epoch: 412 Loss: [0.32294247]\n",
      "Epoch: 413 Loss: [0.32270291]\n",
      "Epoch: 414 Loss: [0.32246256]\n",
      "Epoch: 415 Loss: [0.32222472]\n",
      "Epoch: 416 Loss: [0.32198661]\n",
      "Epoch: 417 Loss: [0.32174889]\n",
      "Epoch: 418 Loss: [0.32151283]\n",
      "Epoch: 419 Loss: [0.32127616]\n",
      "Epoch: 420 Loss: [0.32104303]\n",
      "Epoch: 421 Loss: [0.32080896]\n",
      "Epoch: 422 Loss: [0.32057674]\n",
      "Epoch: 423 Loss: [0.32034435]\n",
      "Epoch: 424 Loss: [0.32011277]\n",
      "Epoch: 425 Loss: [0.31988279]\n",
      "Epoch: 426 Loss: [0.31965231]\n",
      "Epoch: 427 Loss: [0.31942353]\n",
      "Epoch: 428 Loss: [0.31919462]\n",
      "Epoch: 429 Loss: [0.3189674]\n",
      "Epoch: 430 Loss: [0.3187401]\n",
      "Epoch: 431 Loss: [0.3185134]\n",
      "Epoch: 432 Loss: [0.3182883]\n",
      "Epoch: 433 Loss: [0.3180637]\n",
      "Epoch: 434 Loss: [0.31783882]\n",
      "Epoch: 435 Loss: [0.31761574]\n",
      "Epoch: 436 Loss: [0.31739295]\n",
      "Epoch: 437 Loss: [0.31717038]\n",
      "Epoch: 438 Loss: [0.31694838]\n",
      "Epoch: 439 Loss: [0.31672762]\n",
      "Epoch: 440 Loss: [0.31650732]\n",
      "Epoch: 441 Loss: [0.31628669]\n",
      "Epoch: 442 Loss: [0.3160682]\n",
      "Epoch: 443 Loss: [0.31584924]\n",
      "Epoch: 444 Loss: [0.31563083]\n",
      "Epoch: 445 Loss: [0.31541368]\n",
      "Epoch: 446 Loss: [0.31519692]\n",
      "Epoch: 447 Loss: [0.31498022]\n",
      "Epoch: 448 Loss: [0.31476597]\n",
      "Epoch: 449 Loss: [0.31455127]\n",
      "Epoch: 450 Loss: [0.314337]\n",
      "Epoch: 451 Loss: [0.31412413]\n",
      "Epoch: 452 Loss: [0.31391159]\n",
      "Epoch: 453 Loss: [0.31369886]\n",
      "Epoch: 454 Loss: [0.31348793]\n",
      "Epoch: 455 Loss: [0.31327708]\n",
      "Epoch: 456 Loss: [0.31306584]\n",
      "Epoch: 457 Loss: [0.31285624]\n",
      "Epoch: 458 Loss: [0.31264739]\n",
      "Epoch: 459 Loss: [0.31243773]\n",
      "Epoch: 460 Loss: [0.3122303]\n",
      "Epoch: 461 Loss: [0.31202234]\n",
      "Epoch: 462 Loss: [0.31181507]\n",
      "Epoch: 463 Loss: [0.31160788]\n",
      "Epoch: 464 Loss: [0.3114023]\n",
      "Epoch: 465 Loss: [0.31119684]\n",
      "Epoch: 466 Loss: [0.31099166]\n",
      "Epoch: 467 Loss: [0.31078766]\n",
      "Epoch: 468 Loss: [0.31058376]\n",
      "Epoch: 469 Loss: [0.31038062]\n",
      "Epoch: 470 Loss: [0.31017708]\n",
      "Epoch: 471 Loss: [0.30997538]\n",
      "Epoch: 472 Loss: [0.30977384]\n",
      "Epoch: 473 Loss: [0.30957238]\n",
      "Epoch: 474 Loss: [0.3093711]\n",
      "Epoch: 475 Loss: [0.3091716]\n",
      "Epoch: 476 Loss: [0.30897163]\n",
      "Epoch: 477 Loss: [0.30877229]\n",
      "Epoch: 478 Loss: [0.30857317]\n",
      "Epoch: 479 Loss: [0.30837523]\n",
      "Epoch: 480 Loss: [0.30817733]\n",
      "Epoch: 481 Loss: [0.30797991]\n",
      "Epoch: 482 Loss: [0.30778275]\n",
      "Epoch: 483 Loss: [0.30758664]\n",
      "Epoch: 484 Loss: [0.30739062]\n",
      "Epoch: 485 Loss: [0.30719475]\n",
      "Epoch: 486 Loss: [0.30700013]\n",
      "Epoch: 487 Loss: [0.30680666]\n",
      "Epoch: 488 Loss: [0.30661356]\n",
      "Epoch: 489 Loss: [0.30642008]\n",
      "Epoch: 490 Loss: [0.3062283]\n",
      "Epoch: 491 Loss: [0.30603638]\n",
      "Epoch: 492 Loss: [0.30584514]\n",
      "Epoch: 493 Loss: [0.30565347]\n",
      "Epoch: 494 Loss: [0.30546348]\n",
      "Epoch: 495 Loss: [0.30527358]\n",
      "Epoch: 496 Loss: [0.3050842]\n",
      "Epoch: 497 Loss: [0.30489472]\n",
      "Epoch: 498 Loss: [0.30470697]\n",
      "Epoch: 499 Loss: [0.30451876]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 1\n",
    "delta_const = 1e-10\n",
    "num_batches = num_samples/batch_size\n",
    "eps = 1e-3\n",
    "r = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j].reshape(-1,1).T)\n",
    "        r[\"W1\"] = r[\"W1\"] + W1_upd*W1_upd\n",
    "        r[\"W2\"] = r[\"W2\"] + W2_upd*W2_upd\n",
    "        r[\"W3\"] = r[\"W3\"] + W3_upd*W3_upd\n",
    "        r[\"b1\"] = r[\"b1\"] + b1_upd*b1_upd\n",
    "        r[\"b2\"] = r[\"b2\"] + b2_upd*b2_upd\n",
    "        r[\"b3\"] = r[\"b3\"] + b3_upd*b3_upd\n",
    "        W3 = W3 - (eps*W3_upd)/(delta_const + np.sqrt(r[\"W3\"]))\n",
    "        W2 = W2 - (eps*W2_upd)/(delta_const + np.sqrt(r[\"W2\"]))\n",
    "        W1 = W1 - (eps*W1_upd)/(delta_const + np.sqrt(r[\"W1\"]))\n",
    "        b3 = b3 - (eps*b3_upd)/(delta_const + np.sqrt(r[\"b3\"]))\n",
    "        b2 = b2 - (eps*b2_upd)/(delta_const + np.sqrt(r[\"b2\"]))\n",
    "        b1 = b1 - (eps*b1_upd)/(delta_const + np.sqrt(r[\"b1\"]))\n",
    "        \n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01876731]\n",
      " [0.60840382]\n",
      " [0.37282887]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
