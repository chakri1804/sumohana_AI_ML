{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/havish/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y_)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "# Split the data for training and testing\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0\n",
    "    return x\n",
    "def softmax(arr):\n",
    "#     arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "def diff_relu(arr):\n",
    "    z = np.zeros(arr.shape)\n",
    "    z[arr<=0] = 0\n",
    "    z[arr>0] = 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(fan_out,fan_in):\n",
    "    limit = np.sqrt(2*1.0/(fan_in+fan_out))\n",
    "#     return np.random.uniform(-limit,limit,(fan_out,fan_in))\n",
    "    return np.random.normal(0,limit,(fan_out,fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### architecture ###\n",
    "in_dim = 4\n",
    "hid1_dim = 10\n",
    "hid2_dim = 10\n",
    "out_dim = 3\n",
    "W1 = initializer(hid1_dim,in_dim)\n",
    "b1 = initializer(hid1_dim,1)\n",
    "W2 = initializer(hid2_dim,hid1_dim)\n",
    "b2 = initializer(hid2_dim,1)\n",
    "W3 = initializer(out_dim,hid2_dim)\n",
    "b3 = initializer(out_dim,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: [1.7473769]\n",
      "Epoch: 1 Loss: [1.12497395]\n",
      "Epoch: 2 Loss: [0.69787358]\n",
      "Epoch: 3 Loss: [0.47194844]\n",
      "Epoch: 4 Loss: [0.3929087]\n",
      "Epoch: 5 Loss: [0.35021514]\n",
      "Epoch: 6 Loss: [0.31959457]\n",
      "Epoch: 7 Loss: [0.29650163]\n",
      "Epoch: 8 Loss: [0.26944105]\n",
      "Epoch: 9 Loss: [0.2394433]\n",
      "Epoch: 10 Loss: [0.21526301]\n",
      "Epoch: 11 Loss: [0.19657442]\n",
      "Epoch: 12 Loss: [0.18108616]\n",
      "Epoch: 13 Loss: [0.16836384]\n",
      "Epoch: 14 Loss: [0.15769355]\n",
      "Epoch: 15 Loss: [0.148876]\n",
      "Epoch: 16 Loss: [0.14146061]\n",
      "Epoch: 17 Loss: [0.13538122]\n",
      "Epoch: 18 Loss: [0.1302603]\n",
      "Epoch: 19 Loss: [0.12605015]\n",
      "Epoch: 20 Loss: [0.12253361]\n",
      "Epoch: 21 Loss: [0.11974452]\n",
      "Epoch: 22 Loss: [0.11716044]\n",
      "Epoch: 23 Loss: [0.1152773]\n",
      "Epoch: 24 Loss: [0.11359722]\n",
      "Epoch: 25 Loss: [0.11218452]\n",
      "Epoch: 26 Loss: [0.11089571]\n",
      "Epoch: 27 Loss: [0.10998049]\n",
      "Epoch: 28 Loss: [0.10936147]\n",
      "Epoch: 29 Loss: [0.1085618]\n",
      "Epoch: 30 Loss: [0.10789888]\n",
      "Epoch: 31 Loss: [0.10762097]\n",
      "Epoch: 32 Loss: [0.10729053]\n",
      "Epoch: 33 Loss: [0.10702001]\n",
      "Epoch: 34 Loss: [0.10661078]\n",
      "Epoch: 35 Loss: [0.10657855]\n",
      "Epoch: 36 Loss: [0.10641369]\n",
      "Epoch: 37 Loss: [0.10605846]\n",
      "Epoch: 38 Loss: [0.10613605]\n",
      "Epoch: 39 Loss: [0.10604381]\n",
      "Epoch: 40 Loss: [0.10573281]\n",
      "Epoch: 41 Loss: [0.10585744]\n",
      "Epoch: 42 Loss: [0.10556011]\n",
      "Epoch: 43 Loss: [0.10568972]\n",
      "Epoch: 44 Loss: [0.10561161]\n",
      "Epoch: 45 Loss: [0.10527755]\n",
      "Epoch: 46 Loss: [0.10551152]\n",
      "Epoch: 47 Loss: [0.10523905]\n",
      "Epoch: 48 Loss: [0.10546997]\n",
      "Epoch: 49 Loss: [0.10494647]\n",
      "Epoch: 50 Loss: [0.10544076]\n",
      "Epoch: 51 Loss: [0.1052214]\n",
      "Epoch: 52 Loss: [0.1052421]\n",
      "Epoch: 53 Loss: [0.10545527]\n",
      "Epoch: 54 Loss: [0.10526222]\n",
      "Epoch: 55 Loss: [0.1054974]\n",
      "Epoch: 56 Loss: [0.10531257]\n",
      "Epoch: 57 Loss: [0.10535856]\n",
      "Epoch: 58 Loss: [0.10558019]\n",
      "Epoch: 59 Loss: [0.10540165]\n",
      "Epoch: 60 Loss: [0.10565645]\n",
      "Epoch: 61 Loss: [0.10548415]\n",
      "Epoch: 62 Loss: [0.10575145]\n",
      "Epoch: 63 Loss: [0.10558254]\n",
      "Epoch: 64 Loss: [0.10566788]\n",
      "Epoch: 65 Loss: [0.10574184]\n",
      "Epoch: 66 Loss: [0.10571462]\n",
      "Epoch: 67 Loss: [0.10593425]\n",
      "Epoch: 68 Loss: [0.10627923]\n",
      "Epoch: 69 Loss: [0.10614601]\n",
      "Epoch: 70 Loss: [0.10630089]\n",
      "Epoch: 71 Loss: [0.10652327]\n",
      "Epoch: 72 Loss: [0.106674]\n",
      "Epoch: 73 Loss: [0.10669641]\n",
      "Epoch: 74 Loss: [0.10693021]\n",
      "Epoch: 75 Loss: [0.10731521]\n",
      "Epoch: 76 Loss: [0.10723661]\n",
      "Epoch: 77 Loss: [0.1076419]\n",
      "Epoch: 78 Loss: [0.10754507]\n",
      "Epoch: 79 Loss: [0.1077738]\n",
      "Epoch: 80 Loss: [0.10793119]\n",
      "Epoch: 81 Loss: [0.10808345]\n",
      "Epoch: 82 Loss: [0.10825047]\n",
      "Epoch: 83 Loss: [0.10836754]\n",
      "Epoch: 84 Loss: [0.10825581]\n",
      "Epoch: 85 Loss: [0.10879976]\n",
      "Epoch: 86 Loss: [0.10870638]\n",
      "Epoch: 87 Loss: [0.10871556]\n",
      "Epoch: 88 Loss: [0.10905474]\n",
      "Epoch: 89 Loss: [0.10899454]\n",
      "Epoch: 90 Loss: [0.1091817]\n",
      "Epoch: 91 Loss: [0.109322]\n",
      "Epoch: 92 Loss: [0.1093197]\n",
      "Epoch: 93 Loss: [0.10950306]\n",
      "Epoch: 94 Loss: [0.10950872]\n",
      "Epoch: 95 Loss: [0.10945449]\n",
      "Epoch: 96 Loss: [0.1097546]\n",
      "Epoch: 97 Loss: [0.10951471]\n",
      "Epoch: 98 Loss: [0.10966011]\n",
      "Epoch: 99 Loss: [0.10976703]\n",
      "Epoch: 100 Loss: [0.10962064]\n",
      "Epoch: 101 Loss: [0.10956089]\n",
      "Epoch: 102 Loss: [0.10957495]\n",
      "Epoch: 103 Loss: [0.10969655]\n",
      "Epoch: 104 Loss: [0.10959098]\n",
      "Epoch: 105 Loss: [0.10939707]\n",
      "Epoch: 106 Loss: [0.10952368]\n",
      "Epoch: 107 Loss: [0.10949265]\n",
      "Epoch: 108 Loss: [0.10941858]\n",
      "Epoch: 109 Loss: [0.10921266]\n",
      "Epoch: 110 Loss: [0.10922708]\n",
      "Epoch: 111 Loss: [0.10944474]\n",
      "Epoch: 112 Loss: [0.10918659]\n",
      "Epoch: 113 Loss: [0.10907215]\n",
      "Epoch: 114 Loss: [0.10944034]\n",
      "Epoch: 115 Loss: [0.10942291]\n",
      "Epoch: 116 Loss: [0.10941616]\n",
      "Epoch: 117 Loss: [0.10939422]\n",
      "Epoch: 118 Loss: [0.10940353]\n",
      "Epoch: 119 Loss: [0.10933815]\n",
      "Epoch: 120 Loss: [0.10974133]\n",
      "Epoch: 121 Loss: [0.10937459]\n",
      "Epoch: 122 Loss: [0.10941404]\n",
      "Epoch: 123 Loss: [0.10943792]\n",
      "Epoch: 124 Loss: [0.10941746]\n",
      "Epoch: 125 Loss: [0.10931474]\n",
      "Epoch: 126 Loss: [0.10951464]\n",
      "Epoch: 127 Loss: [0.1094889]\n",
      "Epoch: 128 Loss: [0.10943174]\n",
      "Epoch: 129 Loss: [0.10947545]\n",
      "Epoch: 130 Loss: [0.1092783]\n",
      "Epoch: 131 Loss: [0.10951065]\n",
      "Epoch: 132 Loss: [0.10937849]\n",
      "Epoch: 133 Loss: [0.10940171]\n",
      "Epoch: 134 Loss: [0.109362]\n",
      "Epoch: 135 Loss: [0.10933386]\n",
      "Epoch: 136 Loss: [0.10927329]\n",
      "Epoch: 137 Loss: [0.10928578]\n",
      "Epoch: 138 Loss: [0.10923701]\n",
      "Epoch: 139 Loss: [0.10919962]\n",
      "Epoch: 140 Loss: [0.10899758]\n",
      "Epoch: 141 Loss: [0.10919705]\n",
      "Epoch: 142 Loss: [0.1090773]\n",
      "Epoch: 143 Loss: [0.10903947]\n",
      "Epoch: 144 Loss: [0.10899337]\n",
      "Epoch: 145 Loss: [0.10895102]\n",
      "Epoch: 146 Loss: [0.10883703]\n",
      "Epoch: 147 Loss: [0.10904099]\n",
      "Epoch: 148 Loss: [0.1090035]\n",
      "Epoch: 149 Loss: [0.10893315]\n",
      "Epoch: 150 Loss: [0.10887527]\n",
      "Epoch: 151 Loss: [0.10881907]\n",
      "Epoch: 152 Loss: [0.10869176]\n",
      "Epoch: 153 Loss: [0.10891578]\n",
      "Epoch: 154 Loss: [0.10882932]\n",
      "Epoch: 155 Loss: [0.10876109]\n",
      "Epoch: 156 Loss: [0.10869538]\n",
      "Epoch: 157 Loss: [0.10838793]\n",
      "Epoch: 158 Loss: [0.10886625]\n",
      "Epoch: 159 Loss: [0.10868504]\n",
      "Epoch: 160 Loss: [0.10860578]\n",
      "Epoch: 161 Loss: [0.10853446]\n",
      "Epoch: 162 Loss: [0.10839436]\n",
      "Epoch: 163 Loss: [0.10861293]\n",
      "Epoch: 164 Loss: [0.1085142]\n",
      "Epoch: 165 Loss: [0.1084359]\n",
      "Epoch: 166 Loss: [0.10829038]\n",
      "Epoch: 167 Loss: [0.10850751]\n",
      "Epoch: 168 Loss: [0.10840397]\n",
      "Epoch: 169 Loss: [0.10832137]\n",
      "Epoch: 170 Loss: [0.10817245]\n",
      "Epoch: 171 Loss: [0.10838842]\n",
      "Epoch: 172 Loss: [0.10828155]\n",
      "Epoch: 173 Loss: [0.10819612]\n",
      "Epoch: 174 Loss: [0.1080453]\n",
      "Epoch: 175 Loss: [0.10826038]\n",
      "Epoch: 176 Loss: [0.10815137]\n",
      "Epoch: 177 Loss: [0.10806431]\n",
      "Epoch: 178 Loss: [0.10791277]\n",
      "Epoch: 179 Loss: [0.10812724]\n",
      "Epoch: 180 Loss: [0.10801713]\n",
      "Epoch: 181 Loss: [0.10768043]\n",
      "Epoch: 182 Loss: [0.10817045]\n",
      "Epoch: 183 Loss: [0.10795993]\n",
      "Epoch: 184 Loss: [0.10786293]\n",
      "Epoch: 185 Loss: [0.10770825]\n",
      "Epoch: 186 Loss: [0.10792185]\n",
      "Epoch: 187 Loss: [0.10780879]\n",
      "Epoch: 188 Loss: [0.1076505]\n",
      "Epoch: 189 Loss: [0.1078637]\n",
      "Epoch: 190 Loss: [0.10774796]\n",
      "Epoch: 191 Loss: [0.1076546]\n",
      "Epoch: 192 Loss: [0.10749976]\n",
      "Epoch: 193 Loss: [0.10771289]\n",
      "Epoch: 194 Loss: [0.10759901]\n",
      "Epoch: 195 Loss: [0.1074411]\n",
      "Epoch: 196 Loss: [0.10765398]\n",
      "Epoch: 197 Loss: [0.10753811]\n",
      "Epoch: 198 Loss: [0.10737887]\n",
      "Epoch: 199 Loss: [0.10759148]\n",
      "Epoch: 200 Loss: [0.10747424]\n",
      "Epoch: 201 Loss: [0.10737914]\n",
      "Epoch: 202 Loss: [0.10722445]\n",
      "Epoch: 203 Loss: [0.10724283]\n",
      "Epoch: 204 Loss: [0.10742907]\n",
      "Epoch: 205 Loss: [0.10716634]\n",
      "Epoch: 206 Loss: [0.10737315]\n",
      "Epoch: 207 Loss: [0.10725797]\n",
      "Epoch: 208 Loss: [0.10710101]\n",
      "Epoch: 209 Loss: [0.10731353]\n",
      "Epoch: 210 Loss: [0.10719799]\n",
      "Epoch: 211 Loss: [0.1070407]\n",
      "Epoch: 212 Loss: [0.10725309]\n",
      "Epoch: 213 Loss: [0.10713722]\n",
      "Epoch: 214 Loss: [0.10698]\n",
      "Epoch: 215 Loss: [0.10719223]\n",
      "Epoch: 216 Loss: [0.10707633]\n",
      "Epoch: 217 Loss: [0.10671921]\n",
      "Epoch: 218 Loss: [0.10724552]\n",
      "Epoch: 219 Loss: [0.10701557]\n",
      "Epoch: 220 Loss: [0.10691531]\n",
      "Epoch: 221 Loss: [0.10676326]\n",
      "Epoch: 222 Loss: [0.10697535]\n",
      "Epoch: 223 Loss: [0.10686512]\n",
      "Epoch: 224 Loss: [0.10671271]\n",
      "Epoch: 225 Loss: [0.10692513]\n",
      "Epoch: 226 Loss: [0.10681442]\n",
      "Epoch: 227 Loss: [0.10666255]\n",
      "Epoch: 228 Loss: [0.10687493]\n",
      "Epoch: 229 Loss: [0.10676457]\n",
      "Epoch: 230 Loss: [0.10639947]\n",
      "Epoch: 231 Loss: [0.10694783]\n",
      "Epoch: 232 Loss: [0.10671528]\n",
      "Epoch: 233 Loss: [0.10655603]\n",
      "Epoch: 234 Loss: [0.10676988]\n",
      "Epoch: 235 Loss: [0.10666098]\n",
      "Epoch: 236 Loss: [0.10651123]\n",
      "Epoch: 237 Loss: [0.10672338]\n",
      "Epoch: 238 Loss: [0.10661523]\n",
      "Epoch: 239 Loss: [0.10624821]\n",
      "Epoch: 240 Loss: [0.10680381]\n",
      "Epoch: 241 Loss: [0.10657025]\n",
      "Epoch: 242 Loss: [0.10641726]\n",
      "Epoch: 243 Loss: [0.10662906]\n",
      "Epoch: 244 Loss: [0.10652309]\n",
      "Epoch: 245 Loss: [0.10637623]\n",
      "Epoch: 246 Loss: [0.10658373]\n",
      "Epoch: 247 Loss: [0.1064859]\n",
      "Epoch: 248 Loss: [0.1063953]\n",
      "Epoch: 249 Loss: [0.10602511]\n",
      "Epoch: 250 Loss: [0.10659548]\n",
      "Epoch: 251 Loss: [0.10636448]\n",
      "Epoch: 252 Loss: [0.10621792]\n",
      "Epoch: 253 Loss: [0.10643033]\n",
      "Epoch: 254 Loss: [0.1063319]\n",
      "Epoch: 255 Loss: [0.10619079]\n",
      "Epoch: 256 Loss: [0.10639988]\n",
      "Epoch: 257 Loss: [0.10630805]\n",
      "Epoch: 258 Loss: [0.10593097]\n",
      "Epoch: 259 Loss: [0.1065127]\n",
      "Epoch: 260 Loss: [0.10628014]\n",
      "Epoch: 261 Loss: [0.10613667]\n",
      "Epoch: 262 Loss: [0.10634615]\n",
      "Epoch: 263 Loss: [0.10625601]\n",
      "Epoch: 264 Loss: [0.10611612]\n",
      "Epoch: 265 Loss: [0.10632584]\n",
      "Epoch: 266 Loss: [0.10623656]\n",
      "Epoch: 267 Loss: [0.10585593]\n",
      "Epoch: 268 Loss: [0.10644589]\n",
      "Epoch: 269 Loss: [0.10621684]\n",
      "Epoch: 270 Loss: [0.10607565]\n",
      "Epoch: 271 Loss: [0.10628569]\n",
      "Epoch: 272 Loss: [0.10619631]\n",
      "Epoch: 273 Loss: [0.10581478]\n",
      "Epoch: 274 Loss: [0.10641463]\n",
      "Epoch: 275 Loss: [0.10618163]\n",
      "Epoch: 276 Loss: [0.10604651]\n",
      "Epoch: 277 Loss: [0.10625587]\n",
      "Epoch: 278 Loss: [0.10616931]\n",
      "Epoch: 279 Loss: [0.10609438]\n",
      "Epoch: 280 Loss: [0.10570517]\n",
      "Epoch: 281 Loss: [0.10632113]\n",
      "Epoch: 282 Loss: [0.10608818]\n",
      "Epoch: 283 Loss: [0.10595813]\n",
      "Epoch: 284 Loss: [0.10616905]\n",
      "Epoch: 285 Loss: [0.10608871]\n",
      "Epoch: 286 Loss: [0.105696]\n",
      "Epoch: 287 Loss: [0.10632189]\n",
      "Epoch: 288 Loss: [0.10608968]\n",
      "Epoch: 289 Loss: [0.10595957]\n",
      "Epoch: 290 Loss: [0.10617289]\n",
      "Epoch: 291 Loss: [0.10609472]\n",
      "Epoch: 292 Loss: [0.10596787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 293 Loss: [0.10618122]\n",
      "Epoch: 294 Loss: [0.10583019]\n",
      "Epoch: 295 Loss: [0.10613163]\n",
      "Epoch: 296 Loss: [0.10618686]\n",
      "Epoch: 297 Loss: [0.10611059]\n",
      "Epoch: 298 Loss: [0.10604237]\n",
      "Epoch: 299 Loss: [0.10591933]\n",
      "Epoch: 300 Loss: [0.10585351]\n",
      "Epoch: 301 Loss: [0.10621739]\n",
      "Epoch: 302 Loss: [0.10593543]\n",
      "Epoch: 303 Loss: [0.10614888]\n",
      "Epoch: 304 Loss: [0.10607988]\n",
      "Epoch: 305 Loss: [0.10567458]\n",
      "Epoch: 306 Loss: [0.10632984]\n",
      "Epoch: 307 Loss: [0.10610002]\n",
      "Epoch: 308 Loss: [0.10597859]\n",
      "Epoch: 309 Loss: [0.10619326]\n",
      "Epoch: 310 Loss: [0.10612725]\n",
      "Epoch: 311 Loss: [0.10571836]\n",
      "Epoch: 312 Loss: [0.10638162]\n",
      "Epoch: 313 Loss: [0.10615196]\n",
      "Epoch: 314 Loss: [0.10609096]\n",
      "Epoch: 315 Loss: [0.10597385]\n",
      "Epoch: 316 Loss: [0.10618963]\n",
      "Epoch: 317 Loss: [0.10612955]\n",
      "Epoch: 318 Loss: [0.10571737]\n",
      "Epoch: 319 Loss: [0.10638986]\n",
      "Epoch: 320 Loss: [0.10616403]\n",
      "Epoch: 321 Loss: [0.10604865]\n",
      "Epoch: 322 Loss: [0.10626471]\n",
      "Epoch: 323 Loss: [0.10620759]\n",
      "Epoch: 324 Loss: [0.10615253]\n",
      "Epoch: 325 Loss: [0.10573754]\n",
      "Epoch: 326 Loss: [0.10641834]\n",
      "Epoch: 327 Loss: [0.10619656]\n",
      "Epoch: 328 Loss: [0.10577576]\n",
      "Epoch: 329 Loss: [0.10646758]\n",
      "Epoch: 330 Loss: [0.10624338]\n",
      "Epoch: 331 Loss: [0.10613272]\n",
      "Epoch: 332 Loss: [0.10635022]\n",
      "Epoch: 333 Loss: [0.10630025]\n",
      "Epoch: 334 Loss: [0.10625068]\n",
      "Epoch: 335 Loss: [0.10583178]\n",
      "Epoch: 336 Loss: [0.10652843]\n",
      "Epoch: 337 Loss: [0.1063086]\n",
      "Epoch: 338 Loss: [0.10620137]\n",
      "Epoch: 339 Loss: [0.1064204]\n",
      "Epoch: 340 Loss: [0.10605209]\n",
      "Epoch: 341 Loss: [0.10649861]\n",
      "Epoch: 342 Loss: [0.10621652]\n",
      "Epoch: 343 Loss: [0.10643797]\n",
      "Epoch: 344 Loss: [0.10639739]\n",
      "Epoch: 345 Loss: [0.10596668]\n",
      "Epoch: 346 Loss: [0.10668187]\n",
      "Epoch: 347 Loss: [0.1064646]\n",
      "Epoch: 348 Loss: [0.10636211]\n",
      "Epoch: 349 Loss: [0.10658244]\n",
      "Epoch: 350 Loss: [0.10621064]\n",
      "Epoch: 351 Loss: [0.10667625]\n",
      "Epoch: 352 Loss: [0.10639411]\n",
      "Epoch: 353 Loss: [0.10661773]\n",
      "Epoch: 354 Loss: [0.10658377]\n",
      "Epoch: 355 Loss: [0.10654735]\n",
      "Epoch: 356 Loss: [0.10611139]\n",
      "Epoch: 357 Loss: [0.10684127]\n",
      "Epoch: 358 Loss: [0.10663031]\n",
      "Epoch: 359 Loss: [0.10619108]\n",
      "Epoch: 360 Loss: [0.10693117]\n",
      "Epoch: 361 Loss: [0.10671818]\n",
      "Epoch: 362 Loss: [0.10668876]\n",
      "Epoch: 363 Loss: [0.10659186]\n",
      "Epoch: 364 Loss: [0.10681552]\n",
      "Epoch: 365 Loss: [0.1064764]\n",
      "Epoch: 366 Loss: [0.10691664]\n",
      "Epoch: 367 Loss: [0.10665704]\n",
      "Epoch: 368 Loss: [0.10688434]\n",
      "Epoch: 369 Loss: [0.10686235]\n",
      "Epoch: 370 Loss: [0.10683641]\n",
      "Epoch: 371 Loss: [0.10642997]\n",
      "Epoch: 372 Loss: [0.10712485]\n",
      "Epoch: 373 Loss: [0.1069413]\n",
      "Epoch: 374 Loss: [0.1069213]\n",
      "Epoch: 375 Loss: [0.10647294]\n",
      "Epoch: 376 Loss: [0.10723414]\n",
      "Epoch: 377 Loss: [0.10703212]\n",
      "Epoch: 378 Loss: [0.10701564]\n",
      "Epoch: 379 Loss: [0.10656342]\n",
      "Epoch: 380 Loss: [0.10733336]\n",
      "Epoch: 381 Loss: [0.10713264]\n",
      "Epoch: 382 Loss: [0.1071193]\n",
      "Epoch: 383 Loss: [0.10666352]\n",
      "Epoch: 384 Loss: [0.10744185]\n",
      "Epoch: 385 Loss: [0.10724246]\n",
      "Epoch: 386 Loss: [0.10723223]\n",
      "Epoch: 387 Loss: [0.10722014]\n",
      "Epoch: 388 Loss: [0.10676242]\n",
      "Epoch: 389 Loss: [0.10754766]\n",
      "Epoch: 390 Loss: [0.10735272]\n",
      "Epoch: 391 Loss: [0.10734722]\n",
      "Epoch: 392 Loss: [0.10726749]\n",
      "Epoch: 393 Loss: [0.10715604]\n",
      "Epoch: 394 Loss: [0.10765825]\n",
      "Epoch: 395 Loss: [0.10747978]\n",
      "Epoch: 396 Loss: [0.10747866]\n",
      "Epoch: 397 Loss: [0.1070175]\n",
      "Epoch: 398 Loss: [0.10781584]\n",
      "Epoch: 399 Loss: [0.10762665]\n",
      "Epoch: 400 Loss: [0.10762869]\n",
      "Epoch: 401 Loss: [0.10762842]\n",
      "Epoch: 402 Loss: [0.10716757]\n",
      "Epoch: 403 Loss: [0.10797031]\n",
      "Epoch: 404 Loss: [0.10778565]\n",
      "Epoch: 405 Loss: [0.10779213]\n",
      "Epoch: 406 Loss: [0.10779619]\n",
      "Epoch: 407 Loss: [0.10780183]\n",
      "Epoch: 408 Loss: [0.10734438]\n",
      "Epoch: 409 Loss: [0.10814819]\n",
      "Epoch: 410 Loss: [0.10797114]\n",
      "Epoch: 411 Loss: [0.10798339]\n",
      "Epoch: 412 Loss: [0.10763266]\n",
      "Epoch: 413 Loss: [0.10816658]\n",
      "Epoch: 414 Loss: [0.10800659]\n",
      "Epoch: 415 Loss: [0.10754504]\n",
      "Epoch: 416 Loss: [0.10836765]\n",
      "Epoch: 417 Loss: [0.10819399]\n",
      "Epoch: 418 Loss: [0.10821339]\n",
      "Epoch: 419 Loss: [0.10823013]\n",
      "Epoch: 420 Loss: [0.10824831]\n",
      "Epoch: 421 Loss: [0.10785809]\n",
      "Epoch: 422 Loss: [0.10846938]\n",
      "Epoch: 423 Loss: [0.10822511]\n",
      "Epoch: 424 Loss: [0.1080513]\n",
      "Epoch: 425 Loss: [0.10867705]\n",
      "Epoch: 426 Loss: [0.10850573]\n",
      "Epoch: 427 Loss: [0.10853464]\n",
      "Epoch: 428 Loss: [0.10856067]\n",
      "Epoch: 429 Loss: [0.10816956]\n",
      "Epoch: 430 Loss: [0.10879871]\n",
      "Epoch: 431 Loss: [0.10863553]\n",
      "Epoch: 432 Loss: [0.10867069]\n",
      "Epoch: 433 Loss: [0.10870299]\n",
      "Epoch: 434 Loss: [0.1083144]\n",
      "Epoch: 435 Loss: [0.10846291]\n",
      "Epoch: 436 Loss: [0.10900613]\n",
      "Epoch: 437 Loss: [0.1083356]\n",
      "Epoch: 438 Loss: [0.1091772]\n",
      "Epoch: 439 Loss: [0.10902888]\n",
      "Epoch: 440 Loss: [0.1086436]\n",
      "Epoch: 441 Loss: [0.10929375]\n",
      "Epoch: 442 Loss: [0.10864514]\n",
      "Epoch: 443 Loss: [0.10896965]\n",
      "Epoch: 444 Loss: [0.10940352]\n",
      "Epoch: 445 Loss: [0.10925934]\n",
      "Epoch: 446 Loss: [0.10930876]\n",
      "Epoch: 447 Loss: [0.10892179]\n",
      "Epoch: 448 Loss: [0.10958559]\n",
      "Epoch: 449 Loss: [0.1094411]\n",
      "Epoch: 450 Loss: [0.10949509]\n",
      "Epoch: 451 Loss: [0.10910947]\n",
      "Epoch: 452 Loss: [0.10928072]\n",
      "Epoch: 453 Loss: [0.10941008]\n",
      "Epoch: 454 Loss: [0.10983801]\n",
      "Epoch: 455 Loss: [0.10971841]\n",
      "Epoch: 456 Loss: [0.10977856]\n",
      "Epoch: 457 Loss: [0.10942864]\n",
      "Epoch: 458 Loss: [0.10955512]\n",
      "Epoch: 459 Loss: [0.10970866]\n",
      "Epoch: 460 Loss: [0.110187]\n",
      "Epoch: 461 Loss: [0.11005547]\n",
      "Epoch: 462 Loss: [0.1101225]\n",
      "Epoch: 463 Loss: [0.10973647]\n",
      "Epoch: 464 Loss: [0.10992609]\n",
      "Epoch: 465 Loss: [0.11006828]\n",
      "Epoch: 466 Loss: [0.11055756]\n",
      "Epoch: 467 Loss: [0.11043122]\n",
      "Epoch: 468 Loss: [0.10952613]\n",
      "Epoch: 469 Loss: [0.11099408]\n",
      "Epoch: 470 Loss: [0.1101664]\n",
      "Epoch: 471 Loss: [0.11089911]\n",
      "Epoch: 472 Loss: [0.11024382]\n",
      "Epoch: 473 Loss: [0.1111421]\n",
      "Epoch: 474 Loss: [0.1103595]\n",
      "Epoch: 475 Loss: [0.11104909]\n",
      "Epoch: 476 Loss: [0.1109303]\n",
      "Epoch: 477 Loss: [0.1105531]\n",
      "Epoch: 478 Loss: [0.11135696]\n",
      "Epoch: 479 Loss: [0.11054028]\n",
      "Epoch: 480 Loss: [0.11086993]\n",
      "Epoch: 481 Loss: [0.11138371]\n",
      "Epoch: 482 Loss: [0.11134747]\n",
      "Epoch: 483 Loss: [0.11127055]\n",
      "Epoch: 484 Loss: [0.11085286]\n",
      "Epoch: 485 Loss: [0.11106839]\n",
      "Epoch: 486 Loss: [0.11177594]\n",
      "Epoch: 487 Loss: [0.11101702]\n",
      "Epoch: 488 Loss: [0.11169118]\n",
      "Epoch: 489 Loss: [0.11171249]\n",
      "Epoch: 490 Loss: [0.11115616]\n",
      "Epoch: 491 Loss: [0.11134052]\n",
      "Epoch: 492 Loss: [0.11206207]\n",
      "Epoch: 493 Loss: [0.11130223]\n",
      "Epoch: 494 Loss: [0.11199142]\n",
      "Epoch: 495 Loss: [0.11198801]\n",
      "Epoch: 496 Loss: [0.11192424]\n",
      "Epoch: 497 Loss: [0.11100643]\n",
      "Epoch: 498 Loss: [0.11259722]\n",
      "Epoch: 499 Loss: [0.11163311]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "num_samples = len(train_x)\n",
    "batch_size = 1\n",
    "delta_const = 1e-10\n",
    "num_batches = num_samples/batch_size\n",
    "eps = 1e-3\n",
    "rho = 0.4\n",
    "r = {\"W1\" : np.zeros(W1.shape) , \"W2\" : np.zeros(W2.shape) ,\"W3\" : np.zeros(W3.shape),\"b1\": np.zeros(b1.shape)\n",
    "     ,\"b2\" :np.zeros(b2.shape) , \"b3\" : np.zeros(b3.shape)}\n",
    "for i in range(epochs):\n",
    "    (x_train_subs,y_train_subs) = shuffle(train_x,train_y,random_state = 40)\n",
    "    loss = 0\n",
    "    for j in range(int(num_batches)):\n",
    "        W1_upd = np.zeros((hid1_dim,in_dim))\n",
    "        b1_upd = np.zeros((hid1_dim,1))\n",
    "        W2_upd = np.zeros((hid2_dim,hid1_dim))\n",
    "        b2_upd = np.zeros((hid2_dim,1))\n",
    "        W3_upd = np.zeros((out_dim,hid2_dim))\n",
    "        b3_upd = np.zeros((out_dim,1))\n",
    "        for k in range(batch_size):\n",
    "            z1 = relu(np.matmul(W1,x_train_subs[j]).reshape(-1,1)+b1)\n",
    "        \n",
    "            z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "\n",
    "            out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "        \n",
    "            loss = loss + -np.log(out[np.argmax(y_train_subs[j])])\n",
    "        \n",
    "            del_3 = out - y_train_subs[j].reshape(-1,1)\n",
    "            del_2 = np.matmul(W3.T,del_3)*diff_relu(z2)\n",
    "            del_1 = np.matmul(W2.T,del_2)*diff_relu(z1)\n",
    "\n",
    "            b3_upd += del_3\n",
    "#         b3_upd = b3_upd.reshape(len(b3),1)\n",
    "            b2_upd += del_2\n",
    "#         b2_upd = b2_upd.reshape(len(b2),1)\n",
    "            b1_upd += del_1\n",
    "#         b1_upd = b1_upd.reshape(len(b1),1)\n",
    "            W3_upd += np.matmul(del_3,z2.T)\n",
    "            W2_upd += np.matmul(del_2,z1.T)\n",
    "            W1_upd += np.matmul(del_1,x_train_subs[j].reshape(-1,1).T)\n",
    "        r[\"W1\"] = (1-rho)*r[\"W1\"] + rho*W1_upd*W1_upd\n",
    "        r[\"W2\"] = (1-rho)*r[\"W2\"] + rho*W2_upd*W2_upd\n",
    "        r[\"W3\"] = (1-rho)*r[\"W3\"] + rho*W3_upd*W3_upd\n",
    "        r[\"b1\"] = (1-rho)*r[\"b1\"] + rho*b1_upd*b1_upd\n",
    "        r[\"b2\"] = (1-rho)*r[\"b2\"] + rho*b2_upd*b2_upd\n",
    "        r[\"b3\"] = (1-rho)*r[\"b3\"] + rho*b3_upd*b3_upd\n",
    "        W3 = W3 - (eps*W3_upd)/np.sqrt(delta_const + r[\"W3\"])\n",
    "        W2 = W2 - (eps*W2_upd)/np.sqrt(delta_const + r[\"W2\"])\n",
    "        W1 = W1 - (eps*W1_upd)/np.sqrt(delta_const + r[\"W1\"])\n",
    "        b3 = b3 - (eps*b3_upd)/np.sqrt(delta_const + r[\"b3\"])\n",
    "        b2 = b2 - (eps*b2_upd)/np.sqrt(delta_const + r[\"b2\"])\n",
    "        b1 = b1 - (eps*b1_upd)/np.sqrt(delta_const + r[\"b1\"])\n",
    "        \n",
    "    print(\"Epoch: \" + str(i) + \" Loss: \" + str(loss/num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = relu(np.matmul(W1,test_x[4]).reshape(-1,1)+b1)\n",
    "z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18872252e-14]\n",
      " [9.99999864e-01]\n",
      " [1.35838655e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "true = []\n",
    "# print(np.argmax(out))\n",
    "for i in range(len(test_x)):\n",
    "    z1 = relu(np.matmul(W1,test_x[i]).reshape(-1,1)+b1)\n",
    "    z2 = relu(np.matmul(W2,z1).reshape(-1,1)+b2)\n",
    "    out = softmax(np.matmul(W3,z2).reshape(-1,1) + b3)\n",
    "    preds.append(np.argmax(out))\n",
    "    true.append(np.argmax(test_y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99999893e-01]\n",
      " [1.07438936e-07]\n",
      " [8.10283726e-41]]\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
