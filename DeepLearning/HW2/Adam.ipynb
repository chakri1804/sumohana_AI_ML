{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grumptitan/.local/lib/python3.5/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris() \n",
    "x = iris_data.data\n",
    "y_ = iris_data.target.reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y_, test_size=0.20)\n",
    "train_y = encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x[x<0]=0.0\n",
    "    return x\n",
    "\n",
    "def softmax(arr):\n",
    "    arr = arr/np.max(arr)\n",
    "    return np.exp(arr)/(np.sum(np.exp(arr),axis=0))\n",
    "\n",
    "def diff_relu(x):\n",
    "    x[x>0]=1.0\n",
    "    x[x<=0]=0.0\n",
    "    return x\n",
    "\n",
    "def glorot_initializer(out,inp):\n",
    "    limit = np.sqrt(6*1.0/(inp+out))\n",
    "    return np.random.uniform(-limit,limit,(out,inp))\n",
    "\n",
    "def glorot_normal(out,inp):\n",
    "    limit = np.sqrt(2*1.0/(inp+out))\n",
    "    return np.random.normal(0,limit,(out,inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Architecture and weights initializations\n",
    "inp = 4\n",
    "hidden1 = 10\n",
    "hidden2 = 10\n",
    "output = 3\n",
    "\n",
    "W1 = glorot_normal(hidden1,inp)\n",
    "Bi1 = glorot_normal(hidden1,1)\n",
    "W2 = glorot_normal(hidden2,hidden1)\n",
    "Bi2 = glorot_normal(hidden2,1)\n",
    "W3 = glorot_normal(output,hidden2)\n",
    "Bi3 = glorot_normal(output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.378104127729373\n",
      "1 4.36965679280707\n",
      "2 4.360862274407947\n",
      "3 4.354183847268706\n",
      "4 4.346034680015597\n",
      "5 4.339035492374108\n",
      "6 4.33082606702232\n",
      "7 4.323367834787538\n",
      "8 4.31470595054574\n",
      "9 4.306881023036941\n",
      "10 4.298829424957227\n",
      "11 4.292041819769149\n",
      "12 4.283744227699581\n",
      "13 4.276391211393271\n",
      "14 4.267972617899551\n",
      "15 4.2599310628102005\n",
      "16 4.2523252230809465\n",
      "17 4.243374609350607\n",
      "18 4.2356679218467\n",
      "19 4.227495167207048\n",
      "20 4.218942351180894\n",
      "21 4.2113325506662855\n",
      "22 4.200879788914637\n",
      "23 4.1918097170771285\n",
      "24 4.1814341996877475\n",
      "25 4.170422330909153\n",
      "26 4.160861101370539\n",
      "27 4.150629842456359\n",
      "28 4.140370422070384\n",
      "29 4.130539114572956\n",
      "30 4.120612832797992\n",
      "31 4.111252381426879\n",
      "32 4.102671904484993\n",
      "33 4.0938503781837\n",
      "34 4.083144226486038\n",
      "35 4.074537612696919\n",
      "36 4.064495284212141\n",
      "37 4.0553819376168265\n",
      "38 4.045305866078055\n",
      "39 4.036623309840601\n",
      "40 4.02725191941563\n",
      "41 4.020459678712711\n",
      "42 4.0139780100029325\n",
      "43 4.004538884681258\n",
      "44 3.99713931615052\n",
      "45 3.9888786019681857\n",
      "46 3.981078813244728\n",
      "47 3.974122573568251\n",
      "48 3.9674796561048047\n",
      "49 3.9598203254261626\n",
      "50 3.9523294619775386\n",
      "51 3.945654672841151\n",
      "52 3.9386301010640525\n",
      "53 3.933537153526492\n",
      "54 3.929649414501315\n",
      "55 3.922545202571998\n",
      "56 3.9176162689353253\n",
      "57 3.9114820348228836\n",
      "58 3.905694753324055\n",
      "59 3.9007536175819477\n",
      "60 3.895629095982406\n",
      "61 3.891702290076657\n",
      "62 3.890684078754451\n",
      "63 3.8883142747520236\n",
      "64 3.8857785988151803\n",
      "65 3.8819549261244704\n",
      "66 3.878993377598088\n",
      "67 3.876968127393342\n",
      "68 3.8729381497121205\n",
      "69 3.872947967070383\n",
      "70 3.8703194988330423\n",
      "71 3.8687006852543777\n",
      "72 3.8682155772558167\n",
      "73 3.867715032522014\n",
      "74 3.8679604510259864\n",
      "75 3.870427206749216\n",
      "76 3.8703811889780178\n",
      "77 3.8737004350024566\n",
      "78 3.875448201294852\n",
      "79 3.8772875310301056\n",
      "80 3.8806148948115156\n",
      "81 3.8842051942597453\n",
      "82 3.8879865752019165\n",
      "83 3.8918181836061967\n",
      "84 3.895756160657002\n",
      "85 3.900774341018489\n",
      "86 3.9054586721310423\n",
      "87 3.9102621838481846\n",
      "88 3.914928895418451\n",
      "89 3.91890979967465\n",
      "90 3.9223613433193436\n",
      "91 3.9276876001300693\n",
      "92 3.931345821314126\n",
      "93 3.933957366414347\n",
      "94 3.9375053996720895\n",
      "95 3.940648984095414\n",
      "96 3.943712715231812\n",
      "97 3.9467599705967293\n",
      "98 3.9498397636404947\n",
      "99 3.955779007850344\n",
      "100 3.9580964141318913\n",
      "101 3.960684826825817\n",
      "102 3.962836678934367\n",
      "103 3.9658993546851895\n",
      "104 3.9686319699723174\n",
      "105 3.9731682467743132\n",
      "106 3.973417483352178\n",
      "107 3.9776385910902206\n",
      "108 3.980215637896501\n",
      "109 3.9819987344082026\n",
      "110 3.9870853748939443\n",
      "111 3.9901568944567964\n",
      "112 3.9923134049745292\n",
      "113 3.9960194408542367\n",
      "114 3.9962928460409968\n",
      "115 4.000455417829488\n",
      "116 4.001257661794201\n",
      "117 4.002996833530164\n",
      "118 4.006011885571453\n",
      "119 4.006773600807943\n",
      "120 4.010454169929983\n",
      "121 4.014126534877025\n",
      "122 4.014987832868907\n",
      "123 4.016540449398329\n",
      "124 4.020712073789211\n"
     ]
    }
   ],
   "source": [
    "epochs = 125\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1e-8\n",
    "eta = 1e-4\n",
    "\n",
    "f = open(\"Adam.txt\",\"w\")\n",
    "\n",
    "for i in range(epochs):\n",
    "    loss = 0\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    m = [0,0,0,0,0,0]\n",
    "    v = [0,0,0,0,0,0]\n",
    "    m1 = [0,0,0,0,0,0]\n",
    "    v1 = [0,0,0,0,0,0]\n",
    "    for j in range(len(train_x)):\n",
    "        #Forward pass\n",
    "        t = train_x[j]\n",
    "        t1 = train_y[j]\n",
    "        h1 = (np.matmul(W1,t.reshape(-1,1)).reshape(-1,1)+Bi1).reshape(-1,1)\n",
    "        h1_r = (relu(h1)).reshape(-1,1)\n",
    "        h2 = (np.matmul(W2,h1_r).reshape(-1,1)+Bi2).reshape(-1,1)\n",
    "        h2_r = (relu(h2)).reshape(-1,1)\n",
    "        out = (np.matmul(W3,h2_r.reshape(-1,1))+Bi3).reshape(-1,1)\n",
    "        y = (softmax(out)).reshape(-1,1)\n",
    "        \n",
    "        #Backprop\n",
    "        d3 = y - (t1).reshape(-1,1)\n",
    "        d2 = np.matmul(W3.T,d3) * diff_relu(h2)\n",
    "        d1 = np.matmul(W2.T,d2) * diff_relu(h1)\n",
    "                \n",
    "        m[0] = beta1*m[0] + (1-beta1)*d3\n",
    "        m[1] = beta1*m[1] + (1-beta1)*d2\n",
    "        m[2] = beta1*m[2] + (1-beta1)*d1\n",
    "        m[3] = beta1*m[3] + (1-beta1)*np.matmul(d3,h2_r.T)\n",
    "        m[4] = beta1*m[4] + (1-beta1)*np.matmul(d2,h1_r.T)\n",
    "        m[5] = beta1*m[5] + (1-beta1)*np.matmul(d1,t.reshape(-1,1).T)\n",
    "        \n",
    "        v[0] = beta1*v[0] + (1-beta1)*d3**2\n",
    "        v[1] = beta1*v[1] + (1-beta1)*d2**2\n",
    "        v[2] = beta1*v[2] + (1-beta1)*d1**2\n",
    "        v[3] = beta1*v[3] + (1-beta1)*np.matmul(d3,h2_r.T)**2\n",
    "        v[4] = beta1*v[4] + (1-beta1)*np.matmul(d2,h1_r.T)**2\n",
    "        v[5] = beta1*v[5] + (1-beta1)*np.matmul(d1,t.reshape(-1,1).T)**2\n",
    "        \n",
    "        for k in range(6):\n",
    "            m1[k] = m[k]/(1-beta1)\n",
    "            v1[k] = v[k]/(1-beta2)\n",
    "        \n",
    "        Bi3 -= (eta*m1[0])/(np.sqrt(v1[0]) + eps)\n",
    "        Bi2 -= (eta*m1[1])/(np.sqrt(v1[1]) + eps)\n",
    "        Bi1 -= (eta*m1[2])/(np.sqrt(v1[2]) + eps)\n",
    "        W3  -= (eta*m1[3])/(np.sqrt(v1[3]) + eps)\n",
    "        W2  -= (eta*m1[4])/(np.sqrt(v1[4]) + eps)\n",
    "        W1  -= (eta*m1[5])/(np.sqrt(v1[5]) + eps)\n",
    "        \n",
    "        loss -= np.sum(t1*np.log(y))\n",
    "    \n",
    "    print(i,loss/len(train_x))\n",
    "    f.write(\"%s\" %(loss/len(train_x)))\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for j in range(len(test_x)):\n",
    "    t = test_x[j]\n",
    "    h1 = (np.matmul(W1,t.reshape(-1,1)).reshape(-1,1)+Bi1).reshape(-1,1)\n",
    "    h1_r = (relu(h1)).reshape(-1,1)\n",
    "    h2 = (np.matmul(W2,h1_r).reshape(-1,1)+Bi2).reshape(-1,1)\n",
    "    h2_r = (relu(h2)).reshape(-1,1)\n",
    "    out = (np.matmul(W3,h2_r.reshape(-1,1))+Bi3).reshape(-1,1)\n",
    "    y = (softmax(out)).reshape(-1,1)\n",
    "    y_pred.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array(y_pred)\n",
    "a = np.squeeze(a)\n",
    "eww = []\n",
    "for i in a:\n",
    "    eww.append(np.argmax(i))\n",
    "print(np.array(eww))\n",
    "print(np.squeeze(test_y))\n",
    "print(accuracy_score(np.squeeze(test_y), np.array(eww)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
